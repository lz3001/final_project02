{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\"\"\"\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\"\"\"\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a8be8-6430-4ae5-ac26-f0534f906d4a",
   "metadata": {},
   "source": [
    "### 1. Downloading Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "391fde12-5f5e-47fe-ae34-0a9e3dc6b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Yellow Taxi files...\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet. Skipping.\n",
      "Downloading FHVHV files...\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet. Skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_parquet_links(url, regex_pattern):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "    return [link for link in links if re.search(regex_pattern, link)]\n",
    "\n",
    "def download_parquet_files(links, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for link in links:\n",
    "        file_name = os.path.join(save_dir, os.path.basename(link))\n",
    "        response = requests.get(link, stream=True)\n",
    "        with open(file_name, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "# Filter links in date range\n",
    "def filter_links_by_date(links, start_date, end_date):\n",
    "\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        match = re.search(r'(\\d{4})-(\\d{2})', link)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            date = datetime(year, month, 1)\n",
    "            if start_date <= date <= end_date:\n",
    "                filtered_links.append(link)\n",
    "    return filtered_links\n",
    "\n",
    "# Define date range\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2024, 8, 1)\n",
    "\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "yellow_regex = r'yellow_tripdata_\\d{4}-\\d{2}\\.parquet'\n",
    "fhvhv_regex = r'fhvhv_tripdata_\\d{4}-\\d{2}\\.parquet'\n",
    "\n",
    "# Fetch links\n",
    "yellow_links = get_parquet_links(url, yellow_regex)\n",
    "fhvhv_links = get_parquet_links(url, fhvhv_regex)\n",
    "\n",
    "# Filter links by date\n",
    "yellow_links_filtered = filter_links_by_date(yellow_links, start_date, end_date)\n",
    "fhvhv_links_filtered = filter_links_by_date(fhvhv_links, start_date, end_date)\n",
    "\n",
    "# Download filtered files\n",
    "download_parquet_files(yellow_links_filtered, \"yellow_taxi_data\")\n",
    "download_parquet_files(fhvhv_links_filtered, \"fhvhv_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7914d1-4540-4db7-9692-94107d86a256",
   "metadata": {},
   "source": [
    "### 2. Sampling with Cochran's Formula\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d984bbc2-7713-4e3c-928e-d7c4da66506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a52e6-4a02-49e4-b17c-0c5ea71af16b",
   "metadata": {},
   "source": [
    "#### 2.1 Define the Sampling Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dfb0fb3-a5df-4f4c-a6ad-625692adde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cochran_sample_size(population_size, confidence_level=0.95, p=0.5, margin_of_error=0.05):\n",
    "    # Z-scores for common confidence levels\n",
    "    z_scores = {0.9: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    z = z_scores[confidence_level]\n",
    "    \n",
    "    # Cochran's initial sample size\n",
    "    n_0 = (z**2 * p * (1 - p)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust sample size for finite population\n",
    "    if population_size > 0:\n",
    "        n = n_0 / (1 + (n_0 - 1) / population_size)\n",
    "    else:\n",
    "        n = n_0  # Default to initial sample size if population size is unknown\n",
    "    \n",
    "    return math.ceil(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858550f-25c0-41e1-acb3-c38ab23dc2e4",
   "metadata": {},
   "source": [
    "#### 2.2 Sampling for Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b37e8db1-3649-4746-8050-715554bd1ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Taxi - yellow_tripdata_2020-01.parquet: Population size = 6405008, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-02.parquet: Population size = 6299367, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-03.parquet: Population size = 3007687, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-04.parquet: Population size = 238073, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-05.parquet: Population size = 348415, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-06.parquet: Population size = 549797, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-07.parquet: Population size = 800412, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-08.parquet: Population size = 1007286, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-09.parquet: Population size = 1341017, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-10.parquet: Population size = 1681132, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-11.parquet: Population size = 1509000, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-12.parquet: Population size = 1461898, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-01.parquet: Population size = 1369769, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-02.parquet: Population size = 1371709, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-03.parquet: Population size = 1925152, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-04.parquet: Population size = 2171187, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-05.parquet: Population size = 2507109, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-06.parquet: Population size = 2834264, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-07.parquet: Population size = 2821746, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-08.parquet: Population size = 2788757, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-09.parquet: Population size = 2963793, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-10.parquet: Population size = 3463504, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-11.parquet: Population size = 3472949, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-12.parquet: Population size = 3214369, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-01.parquet: Population size = 2463931, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-02.parquet: Population size = 2979431, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-03.parquet: Population size = 3627882, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-04.parquet: Population size = 3599920, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-05.parquet: Population size = 3588295, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-06.parquet: Population size = 3558124, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-07.parquet: Population size = 3174394, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-08.parquet: Population size = 3152677, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-09.parquet: Population size = 3183767, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-10.parquet: Population size = 3675411, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-11.parquet: Population size = 3252717, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-12.parquet: Population size = 3399549, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-01.parquet: Population size = 3066766, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-02.parquet: Population size = 2913955, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-03.parquet: Population size = 3403766, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-04.parquet: Population size = 3288250, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-05.parquet: Population size = 3513649, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-06.parquet: Population size = 3307234, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-07.parquet: Population size = 2907108, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-08.parquet: Population size = 2824209, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-09.parquet: Population size = 2846722, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-10.parquet: Population size = 3522285, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-11.parquet: Population size = 3339715, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-12.parquet: Population size = 3376567, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-01.parquet: Population size = 2964624, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-02.parquet: Population size = 3007526, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-03.parquet: Population size = 3582628, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-04.parquet: Population size = 3514289, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-05.parquet: Population size = 3723833, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-06.parquet: Population size = 3539193, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-07.parquet: Population size = 3076903, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-08.parquet: Population size = 2979183, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-01.parquet: Population size = 20569368, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-02.parquet: Population size = 21725100, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-03.parquet: Population size = 13392928, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-04.parquet: Population size = 4312909, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-05.parquet: Population size = 6089999, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-06.parquet: Population size = 7555193, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-07.parquet: Population size = 9958454, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-08.parquet: Population size = 11096852, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-09.parquet: Population size = 12106669, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-10.parquet: Population size = 13268411, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-11.parquet: Population size = 11596865, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-12.parquet: Population size = 11637123, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-01.parquet: Population size = 11908468, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-02.parquet: Population size = 11613942, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-03.parquet: Population size = 14227393, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-04.parquet: Population size = 14111371, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-05.parquet: Population size = 14719171, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-06.parquet: Population size = 14961892, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-07.parquet: Population size = 15027174, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-08.parquet: Population size = 14499696, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-09.parquet: Population size = 14886055, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-10.parquet: Population size = 16545356, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-11.parquet: Population size = 16041639, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-12.parquet: Population size = 16054495, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-01.parquet: Population size = 14751591, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-02.parquet: Population size = 16019283, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-03.parquet: Population size = 18453548, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-04.parquet: Population size = 17752561, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-05.parquet: Population size = 18157335, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-06.parquet: Population size = 17780075, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-07.parquet: Population size = 17464619, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-08.parquet: Population size = 17185687, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-09.parquet: Population size = 17793551, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-10.parquet: Population size = 19306090, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-11.parquet: Population size = 18085896, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-12.parquet: Population size = 19665847, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-01.parquet: Population size = 18479031, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-02.parquet: Population size = 17960971, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-04.parquet: Population size = 19144903, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-05.parquet: Population size = 19847676, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-06.parquet: Population size = 19366619, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-07.parquet: Population size = 19132131, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-08.parquet: Population size = 18322150, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-09.parquet: Population size = 19851123, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-10.parquet: Population size = 20186330, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-11.parquet: Population size = 19269250, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-12.parquet: Population size = 20516297, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-01.parquet: Population size = 19663930, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-02.parquet: Population size = 19359148, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-03.parquet: Population size = 21280788, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-04.parquet: Population size = 19733038, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-05.parquet: Population size = 20704538, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-06.parquet: Population size = 20123226, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-07.parquet: Population size = 19182934, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-08.parquet: Population size = 19128392, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-08.parquet\n"
     ]
    }
   ],
   "source": [
    "# Function to sample a monthly dataset\n",
    "def sample_monthly_data(file_path, sample_size):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    sampled_df = df.sample(n=sample_size, random_state=42) \n",
    "    return sampled_df\n",
    "\n",
    "# Sampling logic for Yellow Taxi and FHVHV datasets\n",
    "def process_data(data_path_pattern, output_dir, dataset_type):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "    file_paths = sorted(glob.glob(data_path_pattern))  # Get all matching files\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Load dataset to calculate population size\n",
    "        population_size = len(pd.read_parquet(file_path))\n",
    "        \n",
    "        # Calculate sample size\n",
    "        sample_size = cochran_sample_size(population_size, confidence_level=0.95, margin_of_error=0.05)\n",
    "        print(f\"{dataset_type} - {os.path.basename(file_path)}: Population size = {population_size}, Sample size = {sample_size}\")\n",
    "        \n",
    "        # Sample data\n",
    "        sampled_data = sample_monthly_data(file_path, sample_size)\n",
    "        \n",
    "        # Save sampled data\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "        sampled_data.to_parquet(output_file)\n",
    "        print(f\"Sampled data saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# Process both datasets\n",
    "process_data(\"yellow_taxi_data/yellow_tripdata_202*.parquet\", \"yellow_taxi_sampled_data\", \"Yellow Taxi\")\n",
    "process_data(\"fhvhv_data/fhvhv_tripdata_202*.parquet\", \"fhvhv_sampled_data\", \"FHVHV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcfcbb-680e-4f0d-8c56-d830b03823b8",
   "metadata": {},
   "source": [
    "#### 2.3 Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21f807c-b1ab-4594-8b3f-5f8c191b5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import math\n",
    "import requests\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Coordinate Reference System\n",
    "CRS = 4326\n",
    "\n",
    "# (lat, lon) bounding box\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "# Define Uber-specific license number and base numbers\n",
    "uber_license_num = \"HV0003\"\n",
    "uber_base_numbers = [\n",
    "    \"B02877\", \"B02866\", \"B02882\", \"B02869\", \"B02617\", \"B02876\", \"B02865\", \"B02512\", \n",
    "    \"B02888\", \"B02864\", \"B02883\", \"B02875\", \"B02682\", \"B02880\", \"B02870\", \"B02404\", \n",
    "    \"B02598\", \"B02765\", \"B02879\", \"B02867\", \"B02878\", \"B02887\", \"B02872\", \"B02836\", \n",
    "    \"B02884\", \"B02835\", \"B02764\", \"B02889\", \"B02871\", \"B02395\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69bb9298-1b0b-4ec7-93e5-a1accbe2d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned weather data saved to: cleaned_weather_data.parquet\n"
     ]
    }
   ],
   "source": [
    "#!pip install fastparquet\n",
    "#!pip install pyarrow\n",
    "\n",
    "# Load Taxi Zone Shapefile\n",
    "taxi_zones = gpd.read_file(\"taxi_zones.shp\")\n",
    "\n",
    "# Project to a planar CRS before calculating centroids\n",
    "projected_taxi_zones = taxi_zones.to_crs(epsg=3857)  # Web Mercator projection\n",
    "projected_taxi_zones[\"centroid\"] = projected_taxi_zones.geometry.centroid\n",
    "\n",
    "# Reproject centroids back to geographic CRS for latitude/longitude extraction\n",
    "centroids_geographic = projected_taxi_zones[\"centroid\"].to_crs(epsg=4326)\n",
    "taxi_zones[\"latitude\"] = centroids_geographic.y\n",
    "taxi_zones[\"longitude\"] = centroids_geographic.x\n",
    "\n",
    "# Create lookup table for LocationID to Latitude/Longitude\n",
    "location_lookup = taxi_zones[[\"LocationID\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "# Load Weather Data\n",
    "weather_files = [\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2020_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2021_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2022_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2023_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2024_weather.csv\"\n",
    "]\n",
    "\n",
    "# Define data type mapping for weather columns\n",
    "dtype_mapping = {\n",
    "    \"HourlyPrecipitation\": \"string\",\n",
    "    \"DailySnowfall\": \"string\",\n",
    "    \"DailyPrecipitation\": \"string\",\n",
    "    \"LATITUDE\": \"float\",\n",
    "    \"LONGITUDE\": \"float\",\n",
    "    # Add other columns as needed\n",
    "}\n",
    "\n",
    "# Load and concatenate weather data with specified data types\n",
    "weather_data = pd.concat(\n",
    "    [pd.read_csv(file, dtype=dtype_mapping, low_memory=False) for file in weather_files],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Ensure numeric columns are converted to float after handling mixed types\n",
    "numeric_columns = [\"HourlyPrecipitation\", \"DailySnowfall\", \"DailyPrecipitation\"]\n",
    "for col in numeric_columns:\n",
    "    weather_data[col] = pd.to_numeric(weather_data[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "\n",
    "# Keep only the relevant columns\n",
    "weather_columns_to_keep = [\"DATE\", \"LATITUDE\", \"LONGITUDE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\", \"DailySnowfall\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\"]\n",
    "weather_data = weather_data[weather_columns_to_keep]\n",
    "\n",
    "# Rename columns to maintain consistency\n",
    "weather_data.rename(columns={\n",
    "    \"DATE\": \"date\",\n",
    "    \"LATITUDE\": \"latitude\",\n",
    "    \"LONGITUDE\": \"longitude\",\n",
    "    \"HourlyPrecipitation\": \"precipitation_hourly\",\n",
    "    \"DailyPrecipitation\": \"precipitation_daily\",\n",
    "    \"HourlyWindSpeed\": \"wind_speed_hourly\",\n",
    "    \"DailyAverageWindSpeed\": \"wind_speed_daily\",\n",
    "    \"DailySnowfall\": \"snowfall\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert 'T' values to 0.005 in relevant columns\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']].replace('T', 0.005)\n",
    "\n",
    "# Remove any alphabetic characters from numeric columns\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']].replace(r'[a-zA-Z]', '', regex=True)\n",
    "\n",
    "# Convert date column to datetime\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Fill any missing values with zeros (for precipitation, wind speed, snowfall)\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'wind_speed_hourly', 'wind_speed_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'wind_speed_hourly', 'wind_speed_daily', 'snowfall']].fillna(0)\n",
    "\n",
    "# Ensure correct data types\n",
    "weather_data['precipitation_hourly'] = weather_data['precipitation_hourly'].astype(float)\n",
    "weather_data['precipitation_daily'] = weather_data['precipitation_daily'].astype(float)\n",
    "weather_data['wind_speed_hourly'] = weather_data['wind_speed_hourly'].astype(float)\n",
    "weather_data['wind_speed_daily'] = weather_data['wind_speed_daily'].astype(float)\n",
    "weather_data['snowfall'] = weather_data['snowfall'].astype(float)\n",
    "weather_data['latitude'] = weather_data['latitude'].astype(float)\n",
    "weather_data['longitude'] = weather_data['longitude'].astype(float)\n",
    "\n",
    "# Save cleaned weather data to a separate file\n",
    "weather_output_file = \"cleaned_weather_data.parquet\"\n",
    "weather_data.to_parquet(weather_output_file)\n",
    "print(f\"Cleaned weather data saved to: {weather_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39879bfb-c592-4341-8912-26aff6909717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-08.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_and_filter_data(df, location_lookup, dataset_type, weather_data):\n",
    "    \"\"\"\n",
    "    Cleans and filters the ride data, and merges with weather data.\n",
    "    \"\"\"\n",
    "\n",
    "     # Filter Uber rides for FHVHV dataset\n",
    "    if dataset_type == \"FHVHV\":\n",
    "        # Normalize column values by converting to lowercase and stripping whitespaces\n",
    "        df['hvfhs_license_num'] = df['hvfhs_license_num'].str.lower().str.strip()\n",
    "        df['dispatching_base_num'] = df['dispatching_base_num'].str.lower().str.strip()\n",
    "        \n",
    "        # Define Uber-specific license number and base numbers\n",
    "        uber_license_num = \"hv0003\"  # Make sure it's in lowercase\n",
    "        \"\"\"\n",
    "        uber_base_numbers = [\n",
    "            \"b02877\", \"b02866\", \"b02882\", \"b02869\", \"b02617\", \"b02876\", \"b02865\", \"b02512\", \n",
    "            \"b02888\", \"b02864\", \"b02883\", \"b02875\", \"b02682\", \"b02880\", \"b02870\", \"b02404\", \n",
    "            \"b02598\", \"b02765\", \"b02879\", \"b02867\", \"b02878\", \"b02887\", \"b02872\", \"b02836\", \n",
    "            \"b02884\", \"b02835\", \"b02764\", \"b02889\", \"b02871\", \"b02395\"\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # Filter for Uber rides only\n",
    "        df = df[\n",
    "            (df['hvfhs_license_num'] == uber_license_num)]\n",
    "        #&\n",
    "          #  (df['dispatching_base_num'].isin(uber_base_numbers))\n",
    "        \n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "        print(\"Cleaned DataFrame (after filtering uber rides):\")\n",
    "        print(df.head())\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge LocationID with Latitude/Longitude\n",
    "    df = df.merge(location_lookup, left_on=\"PULocationID\", right_on=\"LocationID\", how=\"left\")\n",
    "    df = df.merge(location_lookup, left_on=\"DOLocationID\", right_on=\"LocationID\", how=\"left\", suffixes=(\"_pickup\", \"_dropoff\"))\n",
    "\n",
    "    \n",
    "    # Drop rows with missing latitude/longitude\n",
    "    df = df.dropna(subset=[\"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\"])\n",
    "    \"\"\"\n",
    "    print(\"Cleaned DataFrame (before removing unnecessary):\")\n",
    "    print(df.head())\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove unnecessary columns based on project queries\n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        columns_to_keep = [\n",
    "            \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "            \"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\", \n",
    "            \"fare_amount\", \"trip_distance\", \"total_amount\", \"tip_amount\"\n",
    "        ]\n",
    "        df = df[columns_to_keep]\n",
    "\n",
    "        # Rename columns for Yellow Taxi\n",
    "        df.rename(columns={\n",
    "            \"latitude_pickup\": \"latitude_pickup1\",\n",
    "            \"longitude_pickup\": \"longitude_pickup1\",\n",
    "            \"latitude_dropoff\": \"latitude_dropoff1\",\n",
    "            \"longitude_dropoff\": \"longitude_dropoff1\"\n",
    "        }, inplace=True)\n",
    "\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        columns_to_keep = [\n",
    "            \"pickup_datetime\", \"dropoff_datetime\",\n",
    "            \"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\", \n",
    "            \"base_passenger_fare\", \"trip_miles\", \"tolls\", \"tips\"\n",
    "        ]\n",
    "        df = df[columns_to_keep]\n",
    "\n",
    "        # Rename columns for FHVHV\n",
    "        df.rename(columns={\n",
    "            \"latitude_pickup\": \"latitude_pickup2\",\n",
    "            \"longitude_pickup\": \"longitude_pickup2\",\n",
    "            \"latitude_dropoff\": \"latitude_dropoff2\",\n",
    "            \"longitude_dropoff\": \"longitude_dropoff2\"\n",
    "        }, inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Remove trips with zero distance\n",
    "    #df = df[(df[\"latitude_pickup\"] != df[\"latitude_dropoff\"]) & (df[\"longitude_pickup\"] != df[\"longitude_dropoff\"])]\n",
    "\n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        df = df[\n",
    "            (df[\"latitude_pickup1\"] != df[\"latitude_dropoff1\"]) &\n",
    "            (df[\"longitude_pickup1\"] != df[\"longitude_dropoff1\"])\n",
    "        ]\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        df = df[\n",
    "            (df[\"latitude_pickup2\"] != df[\"latitude_dropoff2\"]) &\n",
    "            (df[\"longitude_pickup2\"] != df[\"longitude_dropoff2\"])\n",
    "        ]\n",
    "    #print(f\"Columns available before filtering: {df.columns.tolist()}\")\n",
    "    \n",
    "\n",
    "    # Remove trips outside of the NYC bounding box\n",
    "    def is_within_bounding_box(lat, lon, bounding_box):\n",
    "        return (bounding_box[0][0] <= lat <= bounding_box[1][0]) and (bounding_box[0][1] <= lon <= bounding_box[1][1])\n",
    "    \n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        df = df[df.apply(lambda row: is_within_bounding_box(row[\"latitude_pickup1\"], row[\"longitude_pickup1\"], NEW_YORK_BOX_COORDS) and\n",
    "                                    is_within_bounding_box(row[\"latitude_dropoff1\"], row[\"longitude_dropoff1\"], NEW_YORK_BOX_COORDS), axis=1)]\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        df = df[df.apply(lambda row: is_within_bounding_box(row[\"latitude_pickup2\"], row[\"longitude_pickup2\"], NEW_YORK_BOX_COORDS) and\n",
    "                                    is_within_bounding_box(row[\"latitude_dropoff2\"], row[\"longitude_dropoff2\"], NEW_YORK_BOX_COORDS), axis=1)]\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Paths for Yellow Taxi and FHVHV data\n",
    "yellow_taxi_path_pattern = \"yellow_taxi_sampled_data/yellow_tripdata_202*.parquet\"\n",
    "fhvhv_path_pattern = \"fhvhv_sampled_data/fhvhv_tripdata_202*.parquet\"\n",
    "\n",
    "# Output directories\n",
    "yellow_taxi_output_dir = \"yellow_taxi_cleaned_data\"\n",
    "fhvhv_output_dir = \"fhvhv_cleaned_data\"\n",
    "\n",
    "# Process both datasets\n",
    "def process_cleaning(data_path_pattern, output_dir, dataset_type, weather_data):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "    file_paths = sorted(glob.glob(data_path_pattern))  # Get all matching files\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        cleaned_data = clean_and_filter_data(df, location_lookup, dataset_type, weather_data)\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "        cleaned_data.to_parquet(output_file)\n",
    "        print(f\"Cleaned data saved to: {output_file}\")\n",
    "\n",
    "# Run the cleaning process\n",
    "process_cleaning(yellow_taxi_path_pattern, yellow_taxi_output_dir, \"Yellow Taxi\", weather_data)\n",
    "process_cleaning(fhvhv_path_pattern, fhvhv_output_dir, \"FHVHV\", weather_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7b69d1e-ab4a-47a1-837c-381d4a4b1a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>precipitation_hourly</th>\n",
       "      <th>wind_speed_hourly</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>precipitation_daily</th>\n",
       "      <th>wind_speed_daily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56093</th>\n",
       "      <td>2024-10-22 14:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56094</th>\n",
       "      <td>2024-10-22 15:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56095</th>\n",
       "      <td>2024-10-22 16:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56096</th>\n",
       "      <td>2024-10-22 17:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56097</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56098 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  latitude  longitude  precipitation_hourly  \\\n",
       "0     2020-01-01 00:51:00  40.77898  -73.96925                   0.0   \n",
       "1     2020-01-01 01:51:00  40.77898  -73.96925                   0.0   \n",
       "2     2020-01-01 02:51:00  40.77898  -73.96925                   0.0   \n",
       "3     2020-01-01 03:51:00  40.77898  -73.96925                   0.0   \n",
       "4     2020-01-01 04:51:00  40.77898  -73.96925                   0.0   \n",
       "...                   ...       ...        ...                   ...   \n",
       "56093 2024-10-22 14:51:00  40.77898  -73.96925                   0.0   \n",
       "56094 2024-10-22 15:51:00  40.77898  -73.96925                   0.0   \n",
       "56095 2024-10-22 16:51:00  40.77898  -73.96925                   0.0   \n",
       "56096 2024-10-22 17:51:00  40.77898  -73.96925                   0.0   \n",
       "56097 2024-10-22 18:51:00  40.77898  -73.96925                   0.0   \n",
       "\n",
       "       wind_speed_hourly  snowfall  precipitation_daily  wind_speed_daily  \n",
       "0                    8.0       0.0                  0.0               0.0  \n",
       "1                    8.0       0.0                  0.0               0.0  \n",
       "2                   14.0       0.0                  0.0               0.0  \n",
       "3                   11.0       0.0                  0.0               0.0  \n",
       "4                    6.0       0.0                  0.0               0.0  \n",
       "...                  ...       ...                  ...               ...  \n",
       "56093                3.0       0.0                  0.0               0.0  \n",
       "56094                0.0       0.0                  0.0               0.0  \n",
       "56095                0.0       0.0                  0.0               0.0  \n",
       "56096                0.0       0.0                  0.0               0.0  \n",
       "56097                0.0       0.0                  0.0               0.0  \n",
       "\n",
       "[56098 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/Users/vivianwang/Desktop/IEOR4501/Final Project/cleaned_weather_data.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdc3c1-b4a6-48eb-9f33-0b393bea263f",
   "metadata": {},
   "source": [
    "### 2.1 Define table schemas using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE hourly_weather (\n",
    "    date TEXT,\n",
    "    latitude REAL,\n",
    "    longitude REAL,\n",
    "    precipitation_hourly REAL,\n",
    "    wind_speed_hourly REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE daily_weather (\n",
    "    date TEXT,\n",
    "    latitude REAL,\n",
    "    longitude REAL,\n",
    "    precipitation_daily REAL,\n",
    "    snowfall REAL,\n",
    "    wind_speed_daily REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE taxi_trips (\n",
    "    pickup_datetime TEXT,\n",
    "    dropoff_datetime TEXT,\n",
    "    latitude_pickup REAL,\n",
    "    longitude_pickup REAL,\n",
    "    latitude_dropoff REAL,\n",
    "    longitude_dropoff REAL,\n",
    "    fare_amount REAL,\n",
    "    trip_distance REAL,\n",
    "    total_amount REAL,\n",
    "    tip_amount REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE uber_trips (\n",
    "    pickup_datetime TEXT,\n",
    "    dropoff_datetime TEXT,\n",
    "    latitude_pickup REAL,\n",
    "    longitude_pickup REAL,\n",
    "    latitude_dropoff REAL,\n",
    "    longitude_dropoff REAL,\n",
    "    base_passenger_fare REAL,\n",
    "    trip_miles REAL,\n",
    "    tolls REAL,\n",
    "    tips REAL\n",
    ");\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema written to schema.sql\n"
     ]
    }
   ],
   "source": [
    "# Write the schemas to a schema.sql file\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "\n",
    "print(f\"Schema written to {DATABASE_SCHEMA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d628b02-5b24-4ee7-bf70-0b47b55fdeb4",
   "metadata": {},
   "source": [
    "### 2.2 Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20120 Yellow Taxi records.\n",
      "Loaded 13756 Uber records.\n",
      "Writing data to table: taxi_trips\n",
      "Data written to table: taxi_trips\n",
      "Writing data to table: uber_trips\n",
      "Data written to table: uber_trips\n",
      "Writing data to table: hourly_weather\n",
      "Data written to table: hourly_weather\n",
      "Writing data to table: daily_weather\n",
      "Data written to table: daily_weather\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom sqlalchemy.sql import text\\n\\n# Verify data in the database\\nwith engine.connect() as connection:\\n    for table in map_table_name_to_dataframe.keys():\\n        # Wrap the query in text() to make it executable\\n        query = text(f\"SELECT COUNT(*) FROM {table}\")\\n        result = connection.execute(query).fetchone()\\n        print(f\"Table {table} contains {result[0]} records.\")\\n\\n    # Optionally, preview a sample of each table\\n    for table in map_table_name_to_dataframe.keys():\\n        query = text(f\"SELECT * FROM {table} LIMIT 5\")\\n        sample = pd.read_sql(query, connection)\\n        print(f\"Sample data from {table}:\")\\n        print(sample)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define database URL and schema file\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "# Combine Yellow Taxi Parquet files\n",
    "yellow_taxi_files = glob.glob(\"yellow_taxi_cleaned_data/*.parquet\")\n",
    "taxi_data = pd.concat([pd.read_parquet(file) for file in yellow_taxi_files], ignore_index=True)\n",
    "print(f\"Loaded {len(taxi_data)} Yellow Taxi records.\")\n",
    "\n",
    "# Combine Uber Parquet files\n",
    "uber_files = glob.glob(\"fhvhv_cleaned_data/*.parquet\")\n",
    "uber_data = pd.concat([pd.read_parquet(file) for file in uber_files], ignore_index=True)\n",
    "print(f\"Loaded {len(uber_data)} Uber records.\")\n",
    "\n",
    "# Load Weather Data\n",
    "weather_data = pd.read_parquet(\"cleaned_weather_data.parquet\")\n",
    "\n",
    "# Split Weather Data\n",
    "hourly_columns = [\"date\", \"latitude\", \"longitude\", \"precipitation_hourly\", \"wind_speed_hourly\"]\n",
    "hourly_data = weather_data[hourly_columns].dropna(subset=[\"precipitation_hourly\", \"wind_speed_hourly\"])\n",
    "\n",
    "daily_columns = [\"date\", \"latitude\", \"longitude\", \"precipitation_daily\", \"snowfall\", \"wind_speed_daily\"]\n",
    "daily_data = weather_data[daily_columns].dropna(subset=[\"precipitation_daily\", \"snowfall\", \"wind_speed_daily\"])\n",
    "\n",
    "# Map table names to DataFrames\n",
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}\n",
    "\n",
    "# Create SQLite Engine\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Function to write DataFrames to tables\n",
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    with engine.connect() as connection:\n",
    "        for table_name, df in table_to_df_dict.items():\n",
    "            print(f\"Writing data to table: {table_name}\")\n",
    "            df.to_sql(table_name, connection, if_exists=\"replace\", index=False)\n",
    "            print(f\"Data written to table: {table_name}\")\n",
    "\n",
    "# Write DataFrames to SQLite database\n",
    "write_dataframes_to_table(map_table_name_to_dataframe)\n",
    "\"\"\"\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "# Verify data in the database\n",
    "with engine.connect() as connection:\n",
    "    for table in map_table_name_to_dataframe.keys():\n",
    "        # Wrap the query in text() to make it executable\n",
    "        query = text(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        result = connection.execute(query).fetchone()\n",
    "        print(f\"Table {table} contains {result[0]} records.\")\n",
    "\n",
    "    # Optionally, preview a sample of each table\n",
    "    for table in map_table_name_to_dataframe.keys():\n",
    "        query = text(f\"SELECT * FROM {table} LIMIT 5\")\n",
    "        sample = pd.read_sql(query, connection)\n",
    "        print(f\"Sample data from {table}:\")\n",
    "        print(sample)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0c350-14b2-4d9b-8d7e-17348b167a94",
   "metadata": {},
   "source": [
    "#### query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adef9826-59b7-47dc-a7f9-74165ead801a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>ride_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>1446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>1291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>1187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>1171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>09</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>08</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>07</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>06</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>01</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>02</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>03</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>05</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>04</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour_of_day  ride_count\n",
       "0           18        1446\n",
       "1           17        1406\n",
       "2           15        1360\n",
       "3           16        1302\n",
       "4           14        1291\n",
       "5           13        1187\n",
       "6           19        1186\n",
       "7           12        1171\n",
       "8           21        1034\n",
       "9           20        1015\n",
       "10          11        1012\n",
       "11          10         985\n",
       "12          09         909\n",
       "13          22         860\n",
       "14          08         813\n",
       "15          23         711\n",
       "16          07         596\n",
       "17          00         496\n",
       "18          06         368\n",
       "19          01         357\n",
       "20          02         206\n",
       "21          03         162\n",
       "22          05         141\n",
       "23          04         106"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite database\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "query_1 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%H', tpep_pickup_datetime) AS hour_of_day,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM \n",
    "    taxi_trips\n",
    "GROUP BY \n",
    "    hour_of_day\n",
    "ORDER BY \n",
    "    ride_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a .sql file\n",
    "with open(\"popular_taxi_hours.sql\", \"w\") as file:\n",
    "    file.write(query_1)\n",
    "\n",
    "# Execute the query and load results into a DataFrame\n",
    "with engine.connect() as connection:\n",
    "    results = pd.read_sql(query_1, connection)\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88e965-3a92-4720-9a74-08d8ad74d527",
   "metadata": {},
   "source": [
    "#### query2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "231903ca-9aaf-428a-a524-e820d1a435a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name</th>\n",
       "      <th>ride_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  day_of_week   day_name  ride_count\n",
       "0           6   Saturday        2342\n",
       "1           5     Friday        2192\n",
       "2           4   Thursday        1966\n",
       "3           0     Sunday        1892\n",
       "4           3  Wednesday        1876\n",
       "5           2    Tuesday        1806\n",
       "6           1     Monday        1682"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "query_2 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    CASE strftime('%w', pickup_datetime)\n",
    "        WHEN '0' THEN 'Sunday'\n",
    "        WHEN '1' THEN 'Monday'\n",
    "        WHEN '2' THEN 'Tuesday'\n",
    "        WHEN '3' THEN 'Wednesday'\n",
    "        WHEN '4' THEN 'Thursday'\n",
    "        WHEN '5' THEN 'Friday'\n",
    "        WHEN '6' THEN 'Saturday'\n",
    "    END AS day_name,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM \n",
    "    uber_trips\n",
    "GROUP BY \n",
    "    day_of_week\n",
    "ORDER BY \n",
    "    ride_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a .sql file\n",
    "with open(\"popular_uber_days.sql\", \"w\") as file:\n",
    "    file.write(query_2)\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as connection:\n",
    "    results = pd.read_sql(query_2, connection)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf70dca-2463-4833-9645-6b96febb24a9",
   "metadata": {},
   "source": [
    "#### query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfcd85ab-9ead-4ad3-bc07-64aaa5820d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Percentile of Trip Distance in January 2024: 16.04\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "query_3 = \"\"\"\n",
    "WITH ordered_distances AS (\n",
    "    SELECT trip_distance\n",
    "    FROM taxi_trips\n",
    "    WHERE strftime('%Y-%m', tpep_pickup_datetime) = '2024-01'\n",
    "    UNION ALL\n",
    "    SELECT trip_miles AS trip_distance\n",
    "    FROM uber_trips\n",
    "    WHERE strftime('%Y-%m', pickup_datetime) = '2024-01'\n",
    "),\n",
    "ranked_distances AS (\n",
    "    SELECT \n",
    "        trip_distance,\n",
    "        ROW_NUMBER() OVER (ORDER BY trip_distance) AS row_num,\n",
    "        COUNT(*) OVER () AS total_rows\n",
    "    FROM ordered_distances\n",
    ")\n",
    "SELECT \n",
    "    trip_distance AS distance_95_percentile\n",
    "FROM ranked_distances\n",
    "WHERE row_num = CAST(0.95 * total_rows AS INTEGER);\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a .sql file\n",
    "with open(\"trip_distance_95_percentile.sql\", \"w\") as file:\n",
    "    file.write(query_3)\n",
    "\n",
    "\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(query_3).fetchone()\n",
    "\n",
    "# Display the results\n",
    "print(\"95% Percentile of Trip Distance in January 2024:\", results[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10936a0-b50d-4590-b0b9-2ac55f004f14",
   "metadata": {},
   "source": [
    "### query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4 = \"\"\"WITH all_rides_2023 AS (\n",
    "    SELECT \n",
    "        DATE(tpep_pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides,\n",
    "        AVG(trip_distance) AS avg_distance\n",
    "    FROM taxi_trips\n",
    "    WHERE DATE(tpep_pickup_datetime) BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    GROUP BY ride_date\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        DATE(pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides,\n",
    "        AVG(trip_miles) AS avg_distance\n",
    "    FROM uber_trips\n",
    "    WHERE DATE(pickup_datetime) BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    GROUP BY ride_date\n",
    "),\n",
    "weather_aggregates AS (\n",
    "    SELECT \n",
    "        DATE(date) AS weather_date,\n",
    "        AVG(precipitation_daily) AS avg_precipitation,\n",
    "        AVG(wind_speed_daily) AS avg_wind_speed\n",
    "    FROM daily_weather\n",
    "    WHERE DATE(date) BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    GROUP BY weather_date\n",
    ")\n",
    "SELECT \n",
    "    a.ride_date AS date,\n",
    "    SUM(a.total_rides) AS total_rides,\n",
    "    AVG(a.avg_distance) AS avg_distance,\n",
    "    w.avg_precipitation,\n",
    "    w.avg_wind_speed\n",
    "FROM all_rides_2023 a\n",
    "LEFT JOIN weather_aggregates w\n",
    "    ON a.ride_date = w.weather_date\n",
    "GROUP BY a.ride_date\n",
    "ORDER BY total_rides DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# Save to a .sql file\n",
    "with open(\"busiest_days_weather.sql\", \"w\") as f:\n",
    "    f.write(query_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"busiest_days_weather.sql\", \"r\") as f:\n",
    "    query_4 = f.read()\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    result_4 = connection.execute(text(query_4)).fetchall()\n",
    "    \n",
    "# Convert to DataFrame\n",
    "columns_4 = [\"date\", \"total_rides\", \"avg_distance\", \"avg_precipitation\", \"avg_wind_speed\"]\n",
    "df_query_4 = pd.DataFrame(result_4, columns=columns_4)\n",
    "print(\"Query 4 Results:\")\n",
    "print(df_query_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa0590-3eb2-4689-b991-fe97002a018c",
   "metadata": {},
   "source": [
    "### query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92337b-15d0-459f-bfb0-daf53a1f6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5 = \"\"\"\n",
    "WITH all_hired_rides AS (\n",
    "    SELECT \n",
    "        DATE(tpep_pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM taxi_trips\n",
    "    WHERE DATE(tpep_pickup_datetime) BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "    GROUP BY ride_date\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        DATE(pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM uber_trips\n",
    "    WHERE DATE(pickup_datetime) BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "    GROUP BY ride_date\n",
    "),\n",
    "daily_snowfall AS (\n",
    "    SELECT \n",
    "        DATE(date) AS snow_date,\n",
    "        SUM(snowfall) AS total_snowfall\n",
    "    FROM daily_weather\n",
    "    WHERE snowfall > 0 AND DATE(date) BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "    GROUP BY snow_date\n",
    ")\n",
    "SELECT \n",
    "    s.snow_date AS date,\n",
    "    s.total_snowfall,\n",
    "    COALESCE(SUM(r.total_rides), 0) AS total_rides\n",
    "FROM daily_snowfall s\n",
    "LEFT JOIN all_hired_rides r\n",
    "    ON s.snow_date = r.ride_date\n",
    "GROUP BY s.snow_date, s.total_snowfall\n",
    "ORDER BY s.total_snowfall DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# Save to a .sql file\n",
    "with open(\"snow_days.sql\", \"w\") as f:\n",
    "    f.write(query_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32da9cf-7edc-4b63-b9b7-05d19b6ac514",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"snow_days.sql\", \"r\") as f:\n",
    "    query_5 = f.read()\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    result_5 = connection.execute(text(query_5)).fetchall()\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns_5 = [\"date\", \"total_snowfall\", \"total_rides\"]\n",
    "df_query_5 = pd.DataFrame(result_5, columns=columns_5)\n",
    "print(\"Query 5 Results:\")\n",
    "print(df_query_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ac710-8639-4c95-ab38-6a00a9d3fb3c",
   "metadata": {},
   "source": [
    "### query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe95d48-7b57-446b-9168-9bc612e8da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_6 = \"\"\"\n",
    "WITH all_hired_rides_hourly AS (\n",
    "    SELECT \n",
    "        STRFTIME('%Y-%m-%d %H:00:00', tpep_pickup_datetime) AS hour,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM taxi_trips\n",
    "    WHERE DATE(tpep_pickup_datetime) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    GROUP BY hour\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        STRFTIME('%Y-%m-%d %H:00:00', pickup_datetime) AS hour,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM uber_trips\n",
    "    WHERE DATE(pickup_datetime) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    GROUP BY hour\n",
    "),\n",
    "weather_hourly AS (\n",
    "    SELECT \n",
    "        STRFTIME('%Y-%m-%d %H:00:00', date) AS hour,\n",
    "        SUM(precipitation_hourly) AS total_precipitation,\n",
    "        AVG(wind_speed_hourly) AS avg_wind_speed\n",
    "    FROM hourly_weather\n",
    "    WHERE DATE(date) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    GROUP BY hour\n",
    "),\n",
    "all_hours AS (\n",
    "    SELECT DISTINCT\n",
    "        STRFTIME('%Y-%m-%d %H:00:00', date) AS hour\n",
    "    FROM hourly_weather\n",
    "    WHERE DATE(date) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    UNION\n",
    "    SELECT DISTINCT\n",
    "        STRFTIME('%Y-%m-%d %H:00:00', tpep_pickup_datetime) AS hour\n",
    "    FROM taxi_trips\n",
    "    WHERE DATE(tpep_pickup_datetime) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    UNION\n",
    "    SELECT DISTINCT\n",
    "        STRFTIME('%Y-%m-%d %H:00:00', pickup_datetime) AS hour\n",
    "    FROM uber_trips\n",
    "    WHERE DATE(pickup_datetime) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    ")\n",
    "SELECT \n",
    "    h.hour,\n",
    "    COALESCE(r.total_rides, 0) AS total_rides,\n",
    "    COALESCE(w.total_precipitation, 0.0) AS total_precipitation,\n",
    "    COALESCE(w.avg_wind_speed, 0.0) AS avg_wind_speed\n",
    "FROM all_hours h\n",
    "LEFT JOIN all_hired_rides_hourly r ON h.hour = r.hour\n",
    "LEFT JOIN weather_hourly w ON h.hour = w.hour\n",
    "WHERE DATE(h.hour) BETWEEN '2023-09-25' AND '2023-10-03'  -- Restrict to exact range\n",
    "ORDER BY h.hour ASC;\n",
    "\"\"\"\n",
    "\n",
    "with open(\"ophelia.sql\", \"w\") as f:\n",
    "    f.write(query_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ae40f-7aae-43f7-b047-ffab4c2379c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "with open(\"ophelia_fixed.sql\", \"w\") as f:\n",
    "    f.write(query_6)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    result_6 = connection.execute(text(query_6)).fetchall()\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns_6 = [\"hour\", \"total_rides\", \"total_precipitation\", \"avg_wind_speed\"]\n",
    "df_query_6 = pd.DataFrame(result_6, columns=columns_6)\n",
    "print(df_query_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
