{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\"\"\"\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\"\"\"\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a8be8-6430-4ae5-ac26-f0534f906d4a",
   "metadata": {},
   "source": [
    "### 1.1 Downloading Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "391fde12-5f5e-47fe-ae34-0a9e3dc6b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Yellow Taxi files...\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet. Skipping.\n",
      "Downloading FHVHV files...\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet. Skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_parquet_links(url, regex_pattern):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "    return [link for link in links if re.search(regex_pattern, link)]\n",
    "\n",
    "def download_parquet_files(links, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for link in links:\n",
    "        file_name = os.path.join(save_dir, os.path.basename(link))\n",
    "        response = requests.get(link, stream=True)\n",
    "        with open(file_name, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "# Filter links in date range\n",
    "def filter_links_by_date(links, start_date, end_date):\n",
    "\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        match = re.search(r'(\\d{4})-(\\d{2})', link)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            date = datetime(year, month, 1)\n",
    "            if start_date <= date <= end_date:\n",
    "                filtered_links.append(link)\n",
    "    return filtered_links\n",
    "\n",
    "# Define date range\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2024, 8, 1)\n",
    "\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "yellow_regex = r'yellow_tripdata_\\d{4}-\\d{2}\\.parquet'\n",
    "fhvhv_regex = r'fhvhv_tripdata_\\d{4}-\\d{2}\\.parquet'\n",
    "\n",
    "# Fetch links\n",
    "yellow_links = get_parquet_links(url, yellow_regex)\n",
    "fhvhv_links = get_parquet_links(url, fhvhv_regex)\n",
    "\n",
    "# Filter links by date\n",
    "yellow_links_filtered = filter_links_by_date(yellow_links, start_date, end_date)\n",
    "fhvhv_links_filtered = filter_links_by_date(fhvhv_links, start_date, end_date)\n",
    "\n",
    "# Download filtered files\n",
    "download_parquet_files(yellow_links_filtered, \"yellow_taxi_data\")\n",
    "download_parquet_files(fhvhv_links_filtered, \"fhvhv_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7914d1-4540-4db7-9692-94107d86a256",
   "metadata": {},
   "source": [
    "### 2. Sampling with Cochran's Formula\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d984bbc2-7713-4e3c-928e-d7c4da66506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a52e6-4a02-49e4-b17c-0c5ea71af16b",
   "metadata": {},
   "source": [
    "#### 2.1 Define the Sampling Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dfb0fb3-a5df-4f4c-a6ad-625692adde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cochran_sample_size(population_size, confidence_level=0.95, p=0.5, margin_of_error=0.05):\n",
    "    # Z-scores for common confidence levels\n",
    "    z_scores = {0.9: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    z = z_scores[confidence_level]\n",
    "    \n",
    "    # Cochran's initial sample size\n",
    "    n_0 = (z**2 * p * (1 - p)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust sample size for finite population\n",
    "    if population_size > 0:\n",
    "        n = n_0 / (1 + (n_0 - 1) / population_size)\n",
    "    else:\n",
    "        n = n_0  # Default to initial sample size if population size is unknown\n",
    "    \n",
    "    return math.ceil(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858550f-25c0-41e1-acb3-c38ab23dc2e4",
   "metadata": {},
   "source": [
    "#### 2.2 Sampling for Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b37e8db1-3649-4746-8050-715554bd1ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Taxi - yellow_tripdata_2020-01.parquet: Population size = 6405008, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-02.parquet: Population size = 6299367, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-03.parquet: Population size = 3007687, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-04.parquet: Population size = 238073, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-05.parquet: Population size = 348415, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-06.parquet: Population size = 549797, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-07.parquet: Population size = 800412, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-08.parquet: Population size = 1007286, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-09.parquet: Population size = 1341017, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-10.parquet: Population size = 1681132, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-11.parquet: Population size = 1509000, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-12.parquet: Population size = 1461898, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-01.parquet: Population size = 1369769, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-02.parquet: Population size = 1371709, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-03.parquet: Population size = 1925152, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-04.parquet: Population size = 2171187, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-05.parquet: Population size = 2507109, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-06.parquet: Population size = 2834264, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-07.parquet: Population size = 2821746, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-08.parquet: Population size = 2788757, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-09.parquet: Population size = 2963793, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-10.parquet: Population size = 3463504, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-11.parquet: Population size = 3472949, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-12.parquet: Population size = 3214369, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-01.parquet: Population size = 2463931, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-02.parquet: Population size = 2979431, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-03.parquet: Population size = 3627882, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-04.parquet: Population size = 3599920, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-05.parquet: Population size = 3588295, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-06.parquet: Population size = 3558124, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-07.parquet: Population size = 3174394, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-08.parquet: Population size = 3152677, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-09.parquet: Population size = 3183767, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-10.parquet: Population size = 3675411, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-11.parquet: Population size = 3252717, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-12.parquet: Population size = 3399549, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-01.parquet: Population size = 3066766, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-02.parquet: Population size = 2913955, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-03.parquet: Population size = 3403766, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-04.parquet: Population size = 3288250, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-05.parquet: Population size = 3513649, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-06.parquet: Population size = 3307234, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-07.parquet: Population size = 2907108, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-08.parquet: Population size = 2824209, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-09.parquet: Population size = 2846722, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-10.parquet: Population size = 3522285, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-11.parquet: Population size = 3339715, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-12.parquet: Population size = 3376567, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-01.parquet: Population size = 2964624, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-02.parquet: Population size = 3007526, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-03.parquet: Population size = 3582628, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-04.parquet: Population size = 3514289, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-05.parquet: Population size = 3723833, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-06.parquet: Population size = 3539193, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-07.parquet: Population size = 3076903, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-08.parquet: Population size = 2979183, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-01.parquet: Population size = 20569368, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-02.parquet: Population size = 21725100, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-03.parquet: Population size = 13392928, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-04.parquet: Population size = 4312909, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-05.parquet: Population size = 6089999, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-06.parquet: Population size = 7555193, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-07.parquet: Population size = 9958454, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-08.parquet: Population size = 11096852, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-09.parquet: Population size = 12106669, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-10.parquet: Population size = 13268411, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-11.parquet: Population size = 11596865, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-12.parquet: Population size = 11637123, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-01.parquet: Population size = 11908468, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-02.parquet: Population size = 11613942, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-03.parquet: Population size = 14227393, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-04.parquet: Population size = 14111371, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-05.parquet: Population size = 14719171, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-06.parquet: Population size = 14961892, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-07.parquet: Population size = 15027174, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-08.parquet: Population size = 14499696, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-09.parquet: Population size = 14886055, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-10.parquet: Population size = 16545356, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-11.parquet: Population size = 16041639, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-12.parquet: Population size = 16054495, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-01.parquet: Population size = 14751591, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-02.parquet: Population size = 16019283, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-03.parquet: Population size = 18453548, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-04.parquet: Population size = 17752561, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-05.parquet: Population size = 18157335, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-06.parquet: Population size = 17780075, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-07.parquet: Population size = 17464619, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-08.parquet: Population size = 17185687, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-09.parquet: Population size = 17793551, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-10.parquet: Population size = 19306090, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-11.parquet: Population size = 18085896, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-12.parquet: Population size = 19665847, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-01.parquet: Population size = 18479031, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-02.parquet: Population size = 17960971, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-04.parquet: Population size = 19144903, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-05.parquet: Population size = 19847676, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-06.parquet: Population size = 19366619, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-07.parquet: Population size = 19132131, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-08.parquet: Population size = 18322150, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-09.parquet: Population size = 19851123, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-10.parquet: Population size = 20186330, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-11.parquet: Population size = 19269250, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-12.parquet: Population size = 20516297, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-01.parquet: Population size = 19663930, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-02.parquet: Population size = 19359148, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-03.parquet: Population size = 21280788, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-04.parquet: Population size = 19733038, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-05.parquet: Population size = 20704538, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-06.parquet: Population size = 20123226, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-07.parquet: Population size = 19182934, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-08.parquet: Population size = 19128392, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-08.parquet\n"
     ]
    }
   ],
   "source": [
    "# Function to sample a monthly dataset\n",
    "def sample_monthly_data(file_path, sample_size):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    sampled_df = df.sample(n=sample_size, random_state=42) \n",
    "    return sampled_df\n",
    "\n",
    "# Sampling logic for Yellow Taxi and FHVHV datasets\n",
    "def process_data(data_path_pattern, output_dir, dataset_type):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "    file_paths = sorted(glob.glob(data_path_pattern))  # Get all matching files\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Load dataset to calculate population size\n",
    "        population_size = len(pd.read_parquet(file_path))\n",
    "        \n",
    "        # Calculate sample size\n",
    "        sample_size = cochran_sample_size(population_size, confidence_level=0.95, margin_of_error=0.05)\n",
    "        print(f\"{dataset_type} - {os.path.basename(file_path)}: Population size = {population_size}, Sample size = {sample_size}\")\n",
    "        \n",
    "        # Sample data\n",
    "        sampled_data = sample_monthly_data(file_path, sample_size)\n",
    "        \n",
    "        # Save sampled data\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "        sampled_data.to_parquet(output_file)\n",
    "        print(f\"Sampled data saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# Process both datasets\n",
    "process_data(\"yellow_taxi_data/yellow_tripdata_202*.parquet\", \"yellow_taxi_sampled_data\", \"Yellow Taxi\")\n",
    "process_data(\"fhvhv_data/fhvhv_tripdata_202*.parquet\", \"fhvhv_sampled_data\", \"FHVHV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcfcbb-680e-4f0d-8c56-d830b03823b8",
   "metadata": {},
   "source": [
    "#### 2.3 Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21f807c-b1ab-4594-8b3f-5f8c191b5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import math\n",
    "import requests\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Coordinate Reference System\n",
    "CRS = 4326\n",
    "\n",
    "# (lat, lon) bounding box\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "# Define Uber-specific license number and base numbers\n",
    "uber_license_num = \"HV0003\"\n",
    "uber_base_numbers = [\n",
    "    \"B02877\", \"B02866\", \"B02882\", \"B02869\", \"B02617\", \"B02876\", \"B02865\", \"B02512\", \n",
    "    \"B02888\", \"B02864\", \"B02883\", \"B02875\", \"B02682\", \"B02880\", \"B02870\", \"B02404\", \n",
    "    \"B02598\", \"B02765\", \"B02879\", \"B02867\", \"B02878\", \"B02887\", \"B02872\", \"B02836\", \n",
    "    \"B02884\", \"B02835\", \"B02764\", \"B02889\", \"B02871\", \"B02395\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69bb9298-1b0b-4ec7-93e5-a1accbe2d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned weather data saved to: cleaned_weather_data.parquet\n"
     ]
    }
   ],
   "source": [
    "#!pip install fastparquet\n",
    "#!pip install pyarrow\n",
    "\n",
    "# Load Taxi Zone Shapefile\n",
    "taxi_zones = gpd.read_file(\"taxi_zones.shp\")\n",
    "\n",
    "# Project to a planar CRS before calculating centroids\n",
    "projected_taxi_zones = taxi_zones.to_crs(epsg=3857)  # Web Mercator projection\n",
    "projected_taxi_zones[\"centroid\"] = projected_taxi_zones.geometry.centroid\n",
    "\n",
    "# Reproject centroids back to geographic CRS for latitude/longitude extraction\n",
    "centroids_geographic = projected_taxi_zones[\"centroid\"].to_crs(epsg=4326)\n",
    "taxi_zones[\"latitude\"] = centroids_geographic.y\n",
    "taxi_zones[\"longitude\"] = centroids_geographic.x\n",
    "\n",
    "# Create lookup table for LocationID to Latitude/Longitude\n",
    "location_lookup = taxi_zones[[\"LocationID\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "# Load Weather Data\n",
    "weather_files = [\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2020_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2021_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2022_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2023_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2024_weather.csv\"\n",
    "]\n",
    "\n",
    "# Define data type mapping for weather columns\n",
    "dtype_mapping = {\n",
    "    \"HourlyPrecipitation\": \"string\",\n",
    "    \"DailySnowfall\": \"string\",\n",
    "    \"DailyPrecipitation\": \"string\",\n",
    "    \"LATITUDE\": \"float\",\n",
    "    \"LONGITUDE\": \"float\",\n",
    "    # Add other columns as needed\n",
    "}\n",
    "\n",
    "# Load and concatenate weather data with specified data types\n",
    "weather_data = pd.concat(\n",
    "    [pd.read_csv(file, dtype=dtype_mapping, low_memory=False) for file in weather_files],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Ensure numeric columns are converted to float after handling mixed types\n",
    "numeric_columns = [\"HourlyPrecipitation\", \"DailySnowfall\", \"DailyPrecipitation\"]\n",
    "for col in numeric_columns:\n",
    "    weather_data[col] = pd.to_numeric(weather_data[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "\n",
    "# Keep only the relevant columns\n",
    "weather_columns_to_keep = [\"DATE\", \"LATITUDE\", \"LONGITUDE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\", \"DailySnowfall\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\"]\n",
    "weather_data = weather_data[weather_columns_to_keep]\n",
    "\n",
    "# Rename columns to maintain consistency\n",
    "weather_data.rename(columns={\n",
    "    \"DATE\": \"date\",\n",
    "    \"LATITUDE\": \"latitude\",\n",
    "    \"LONGITUDE\": \"longitude\",\n",
    "    \"HourlyPrecipitation\": \"precipitation_hourly\",\n",
    "    \"DailyPrecipitation\": \"precipitation_daily\",\n",
    "    \"HourlyWindSpeed\": \"wind_speed_hourly\",\n",
    "    \"DailyAverageWindSpeed\": \"wind_speed_daily\",\n",
    "    \"DailySnowfall\": \"snowfall\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert 'T' values to 0.005 in relevant columns\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']].replace('T', 0.005)\n",
    "\n",
    "# Remove any alphabetic characters from numeric columns\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']].replace(r'[a-zA-Z]', '', regex=True)\n",
    "\n",
    "# Convert date column to datetime\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Fill any missing values with zeros (for precipitation, wind speed, snowfall)\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'wind_speed_hourly', 'wind_speed_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'wind_speed_hourly', 'wind_speed_daily', 'snowfall']].fillna(0)\n",
    "\n",
    "# Ensure correct data types\n",
    "weather_data['precipitation_hourly'] = weather_data['precipitation_hourly'].astype(float)\n",
    "weather_data['precipitation_daily'] = weather_data['precipitation_daily'].astype(float)\n",
    "weather_data['wind_speed_hourly'] = weather_data['wind_speed_hourly'].astype(float)\n",
    "weather_data['wind_speed_daily'] = weather_data['wind_speed_daily'].astype(float)\n",
    "weather_data['snowfall'] = weather_data['snowfall'].astype(float)\n",
    "weather_data['latitude'] = weather_data['latitude'].astype(float)\n",
    "weather_data['longitude'] = weather_data['longitude'].astype(float)\n",
    "\n",
    "# Save cleaned weather data to a separate file\n",
    "weather_output_file = \"cleaned_weather_data.parquet\"\n",
    "weather_data.to_parquet(weather_output_file)\n",
    "print(f\"Cleaned weather data saved to: {weather_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39879bfb-c592-4341-8912-26aff6909717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-08.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_and_filter_data(df, location_lookup, dataset_type, weather_data):\n",
    "    \"\"\"\n",
    "    Cleans and filters the ride data, and merges with weather data.\n",
    "    \"\"\"\n",
    "\n",
    "     # Filter Uber rides for FHVHV dataset\n",
    "    if dataset_type == \"FHVHV\":\n",
    "        # Normalize column values by converting to lowercase and stripping whitespaces\n",
    "        df['hvfhs_license_num'] = df['hvfhs_license_num'].str.lower().str.strip()\n",
    "        df['dispatching_base_num'] = df['dispatching_base_num'].str.lower().str.strip()\n",
    "        \n",
    "        # Define Uber-specific license number and base numbers\n",
    "        uber_license_num = \"hv0003\"  # Make sure it's in lowercase\n",
    "        \"\"\"\n",
    "        uber_base_numbers = [\n",
    "            \"b02877\", \"b02866\", \"b02882\", \"b02869\", \"b02617\", \"b02876\", \"b02865\", \"b02512\", \n",
    "            \"b02888\", \"b02864\", \"b02883\", \"b02875\", \"b02682\", \"b02880\", \"b02870\", \"b02404\", \n",
    "            \"b02598\", \"b02765\", \"b02879\", \"b02867\", \"b02878\", \"b02887\", \"b02872\", \"b02836\", \n",
    "            \"b02884\", \"b02835\", \"b02764\", \"b02889\", \"b02871\", \"b02395\"\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # Filter for Uber rides only\n",
    "        df = df[\n",
    "            (df['hvfhs_license_num'] == uber_license_num)]\n",
    "        #&\n",
    "          #  (df['dispatching_base_num'].isin(uber_base_numbers))\n",
    "        \n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "        print(\"Cleaned DataFrame (after filtering uber rides):\")\n",
    "        print(df.head())\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge LocationID with Latitude/Longitude\n",
    "    df = df.merge(location_lookup, left_on=\"PULocationID\", right_on=\"LocationID\", how=\"left\")\n",
    "    df = df.merge(location_lookup, left_on=\"DOLocationID\", right_on=\"LocationID\", how=\"left\", suffixes=(\"_pickup\", \"_dropoff\"))\n",
    "\n",
    "    \n",
    "    # Drop rows with missing latitude/longitude\n",
    "    df = df.dropna(subset=[\"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\"])\n",
    "    \"\"\"\n",
    "    print(\"Cleaned DataFrame (before removing unnecessary):\")\n",
    "    print(df.head())\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove unnecessary columns based on project queries\n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        columns_to_keep = [\n",
    "            \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "            \"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\", \n",
    "            \"fare_amount\", \"trip_distance\", \"total_amount\", \"tip_amount\"\n",
    "        ]\n",
    "        df = df[columns_to_keep]\n",
    "\n",
    "        # Rename columns for Yellow Taxi\n",
    "        df.rename(columns={\n",
    "            \"latitude_pickup\": \"latitude_pickup1\",\n",
    "            \"longitude_pickup\": \"longitude_pickup1\",\n",
    "            \"latitude_dropoff\": \"latitude_dropoff1\",\n",
    "            \"longitude_dropoff\": \"longitude_dropoff1\"\n",
    "        }, inplace=True)\n",
    "\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        columns_to_keep = [\n",
    "            \"pickup_datetime\", \"dropoff_datetime\",\n",
    "            \"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\", \n",
    "            \"base_passenger_fare\", \"trip_miles\", \"tolls\", \"tips\"\n",
    "        ]\n",
    "        df = df[columns_to_keep]\n",
    "\n",
    "        # Rename columns for FHVHV\n",
    "        df.rename(columns={\n",
    "            \"latitude_pickup\": \"latitude_pickup2\",\n",
    "            \"longitude_pickup\": \"longitude_pickup2\",\n",
    "            \"latitude_dropoff\": \"latitude_dropoff2\",\n",
    "            \"longitude_dropoff\": \"longitude_dropoff2\"\n",
    "        }, inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Remove trips with zero distance\n",
    "    #df = df[(df[\"latitude_pickup\"] != df[\"latitude_dropoff\"]) & (df[\"longitude_pickup\"] != df[\"longitude_dropoff\"])]\n",
    "\n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        df = df[\n",
    "            (df[\"latitude_pickup1\"] != df[\"latitude_dropoff1\"]) &\n",
    "            (df[\"longitude_pickup1\"] != df[\"longitude_dropoff1\"])\n",
    "        ]\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        df = df[\n",
    "            (df[\"latitude_pickup2\"] != df[\"latitude_dropoff2\"]) &\n",
    "            (df[\"longitude_pickup2\"] != df[\"longitude_dropoff2\"])\n",
    "        ]\n",
    "    #print(f\"Columns available before filtering: {df.columns.tolist()}\")\n",
    "    \n",
    "\n",
    "    # Remove trips outside of the NYC bounding box\n",
    "    def is_within_bounding_box(lat, lon, bounding_box):\n",
    "        return (bounding_box[0][0] <= lat <= bounding_box[1][0]) and (bounding_box[0][1] <= lon <= bounding_box[1][1])\n",
    "    \n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        df = df[df.apply(lambda row: is_within_bounding_box(row[\"latitude_pickup1\"], row[\"longitude_pickup1\"], NEW_YORK_BOX_COORDS) and\n",
    "                                    is_within_bounding_box(row[\"latitude_dropoff1\"], row[\"longitude_dropoff1\"], NEW_YORK_BOX_COORDS), axis=1)]\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        df = df[df.apply(lambda row: is_within_bounding_box(row[\"latitude_pickup2\"], row[\"longitude_pickup2\"], NEW_YORK_BOX_COORDS) and\n",
    "                                    is_within_bounding_box(row[\"latitude_dropoff2\"], row[\"longitude_dropoff2\"], NEW_YORK_BOX_COORDS), axis=1)]\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Paths for Yellow Taxi and FHVHV data\n",
    "yellow_taxi_path_pattern = \"yellow_taxi_sampled_data/yellow_tripdata_202*.parquet\"\n",
    "fhvhv_path_pattern = \"fhvhv_sampled_data/fhvhv_tripdata_202*.parquet\"\n",
    "\n",
    "# Output directories\n",
    "yellow_taxi_output_dir = \"yellow_taxi_cleaned_data\"\n",
    "fhvhv_output_dir = \"fhvhv_cleaned_data\"\n",
    "\n",
    "# Process both datasets\n",
    "def process_cleaning(data_path_pattern, output_dir, dataset_type, weather_data):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "    file_paths = sorted(glob.glob(data_path_pattern))  # Get all matching files\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        cleaned_data = clean_and_filter_data(df, location_lookup, dataset_type, weather_data)\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "        cleaned_data.to_parquet(output_file)\n",
    "        print(f\"Cleaned data saved to: {output_file}\")\n",
    "\n",
    "# Run the cleaning process\n",
    "process_cleaning(yellow_taxi_path_pattern, yellow_taxi_output_dir, \"Yellow Taxi\", weather_data)\n",
    "process_cleaning(fhvhv_path_pattern, fhvhv_output_dir, \"FHVHV\", weather_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7b69d1e-ab4a-47a1-837c-381d4a4b1a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>precipitation_hourly</th>\n",
       "      <th>wind_speed_hourly</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>precipitation_daily</th>\n",
       "      <th>wind_speed_daily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56093</th>\n",
       "      <td>2024-10-22 14:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56094</th>\n",
       "      <td>2024-10-22 15:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56095</th>\n",
       "      <td>2024-10-22 16:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56096</th>\n",
       "      <td>2024-10-22 17:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56097</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56098 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  latitude  longitude  precipitation_hourly  \\\n",
       "0     2020-01-01 00:51:00  40.77898  -73.96925                   0.0   \n",
       "1     2020-01-01 01:51:00  40.77898  -73.96925                   0.0   \n",
       "2     2020-01-01 02:51:00  40.77898  -73.96925                   0.0   \n",
       "3     2020-01-01 03:51:00  40.77898  -73.96925                   0.0   \n",
       "4     2020-01-01 04:51:00  40.77898  -73.96925                   0.0   \n",
       "...                   ...       ...        ...                   ...   \n",
       "56093 2024-10-22 14:51:00  40.77898  -73.96925                   0.0   \n",
       "56094 2024-10-22 15:51:00  40.77898  -73.96925                   0.0   \n",
       "56095 2024-10-22 16:51:00  40.77898  -73.96925                   0.0   \n",
       "56096 2024-10-22 17:51:00  40.77898  -73.96925                   0.0   \n",
       "56097 2024-10-22 18:51:00  40.77898  -73.96925                   0.0   \n",
       "\n",
       "       wind_speed_hourly  snowfall  precipitation_daily  wind_speed_daily  \n",
       "0                    8.0       0.0                  0.0               0.0  \n",
       "1                    8.0       0.0                  0.0               0.0  \n",
       "2                   14.0       0.0                  0.0               0.0  \n",
       "3                   11.0       0.0                  0.0               0.0  \n",
       "4                    6.0       0.0                  0.0               0.0  \n",
       "...                  ...       ...                  ...               ...  \n",
       "56093                3.0       0.0                  0.0               0.0  \n",
       "56094                0.0       0.0                  0.0               0.0  \n",
       "56095                0.0       0.0                  0.0               0.0  \n",
       "56096                0.0       0.0                  0.0               0.0  \n",
       "56097                0.0       0.0                  0.0               0.0  \n",
       "\n",
       "[56098 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/Users/vivianwang/Desktop/IEOR4501/Final Project/cleaned_weather_data.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdc3c1-b4a6-48eb-9f33-0b393bea263f",
   "metadata": {},
   "source": [
    "### 2.1 Define table schemas using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE hourly_weather (\n",
    "    date TEXT,\n",
    "    latitude REAL,\n",
    "    longitude REAL,\n",
    "    precipitation_hourly REAL,\n",
    "    wind_speed_hourly REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE daily_weather (\n",
    "    date TEXT,\n",
    "    latitude REAL,\n",
    "    longitude REAL,\n",
    "    precipitation_daily REAL,\n",
    "    snowfall REAL,\n",
    "    wind_speed_daily REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE taxi_trips (\n",
    "    pickup_datetime TEXT,\n",
    "    dropoff_datetime TEXT,\n",
    "    latitude_pickup REAL,\n",
    "    longitude_pickup REAL,\n",
    "    latitude_dropoff REAL,\n",
    "    longitude_dropoff REAL,\n",
    "    fare_amount REAL,\n",
    "    trip_distance REAL,\n",
    "    total_amount REAL,\n",
    "    tip_amount REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE uber_trips (\n",
    "    pickup_datetime TEXT,\n",
    "    dropoff_datetime TEXT,\n",
    "    latitude_pickup REAL,\n",
    "    longitude_pickup REAL,\n",
    "    latitude_dropoff REAL,\n",
    "    longitude_dropoff REAL,\n",
    "    base_passenger_fare REAL,\n",
    "    trip_miles REAL,\n",
    "    tolls REAL,\n",
    "    tips REAL\n",
    ");\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema written to schema.sql\n"
     ]
    }
   ],
   "source": [
    "# Write the schemas to a schema.sql file\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "\n",
    "print(f\"Schema written to {DATABASE_SCHEMA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d628b02-5b24-4ee7-bf70-0b47b55fdeb4",
   "metadata": {},
   "source": [
    "### 2.2 Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20120 Yellow Taxi records.\n",
      "Loaded 13756 Uber records.\n",
      "Writing data to table: taxi_trips\n",
      "Data written to table: taxi_trips\n",
      "Writing data to table: uber_trips\n",
      "Data written to table: uber_trips\n",
      "Writing data to table: hourly_weather\n",
      "Data written to table: hourly_weather\n",
      "Writing data to table: daily_weather\n",
      "Data written to table: daily_weather\n",
      "Table taxi_trips contains 20120 records.\n",
      "Table uber_trips contains 13756 records.\n",
      "Table hourly_weather contains 56098 records.\n",
      "Table daily_weather contains 56098 records.\n",
      "Sample data from taxi_trips:\n",
      "         tpep_pickup_datetime       tpep_dropoff_datetime  latitude_pickup1  \\\n",
      "0  2023-06-28 08:27:09.000000  2023-06-28 09:21:52.000000         40.774376   \n",
      "1  2023-06-13 22:05:38.000000  2023-06-13 22:10:48.000000         40.778766   \n",
      "2  2023-06-09 10:25:49.000000  2023-06-09 10:41:51.000000         40.756688   \n",
      "3  2023-06-28 15:56:14.000000  2023-06-28 17:22:03.000000         40.756729   \n",
      "4  2023-06-22 07:12:42.000000  2023-06-22 07:23:47.000000         40.782480   \n",
      "\n",
      "   longitude_pickup1  latitude_dropoff1  longitude_dropoff1  fare_amount  \\\n",
      "0         -73.873629          40.720889          -73.996919         57.6   \n",
      "1         -73.951010          40.768615          -73.965634          7.2   \n",
      "2         -73.972356          40.780437          -73.957012         15.6   \n",
      "3         -73.965146          40.691833          -74.174000        114.7   \n",
      "4         -73.965553          40.801170          -73.937346         11.4   \n",
      "\n",
      "   trip_distance  total_amount  tip_amount  \n",
      "0          10.22         85.44       17.09  \n",
      "1           0.80         12.20        0.00  \n",
      "2           1.62         21.60        2.00  \n",
      "3          18.90        166.10       27.65  \n",
      "4           1.52         14.84        1.94  \n",
      "Sample data from uber_trips:\n",
      "              pickup_datetime            dropoff_datetime  latitude_pickup2  \\\n",
      "0  2021-03-15 15:23:23.000000  2021-03-15 15:33:05.000000         40.586788   \n",
      "1  2021-03-31 16:30:03.000000  2021-03-31 16:46:07.000000         40.691203   \n",
      "2  2021-03-05 19:47:00.000000  2021-03-05 20:10:56.000000         40.624837   \n",
      "3  2021-03-25 17:16:27.000000  2021-03-25 17:28:13.000000         40.734463   \n",
      "4  2021-03-12 21:06:25.000000  2021-03-12 21:27:46.000000         40.753513   \n",
      "\n",
      "   longitude_pickup2  latitude_dropoff2  longitude_dropoff2  \\\n",
      "0         -74.085512          40.618771          -74.073705   \n",
      "1         -73.763146          40.662185          -73.764506   \n",
      "2         -74.029892          40.599955          -73.964334   \n",
      "3         -73.777253          40.768352          -73.809546   \n",
      "4         -73.988787          40.709139          -74.013023   \n",
      "\n",
      "   base_passenger_fare  trip_miles  tolls  tips  \n",
      "0                10.90        3.20    0.0   0.0  \n",
      "1                15.96        3.51    0.0   0.0  \n",
      "2                27.93        5.81    0.0   0.0  \n",
      "3                13.73        2.33    0.0   0.0  \n",
      "4                20.20        6.55    0.0   0.0  \n",
      "Sample data from hourly_weather:\n",
      "                         date  latitude  longitude  precipitation_hourly  \\\n",
      "0  2020-01-01 00:51:00.000000  40.77898  -73.96925                   0.0   \n",
      "1  2020-01-01 01:51:00.000000  40.77898  -73.96925                   0.0   \n",
      "2  2020-01-01 02:51:00.000000  40.77898  -73.96925                   0.0   \n",
      "3  2020-01-01 03:51:00.000000  40.77898  -73.96925                   0.0   \n",
      "4  2020-01-01 04:51:00.000000  40.77898  -73.96925                   0.0   \n",
      "\n",
      "   wind_speed_hourly  \n",
      "0                8.0  \n",
      "1                8.0  \n",
      "2               14.0  \n",
      "3               11.0  \n",
      "4                6.0  \n",
      "Sample data from daily_weather:\n",
      "                         date  latitude  longitude  precipitation_daily  \\\n",
      "0  2020-01-01 00:51:00.000000  40.77898  -73.96925                  0.0   \n",
      "1  2020-01-01 01:51:00.000000  40.77898  -73.96925                  0.0   \n",
      "2  2020-01-01 02:51:00.000000  40.77898  -73.96925                  0.0   \n",
      "3  2020-01-01 03:51:00.000000  40.77898  -73.96925                  0.0   \n",
      "4  2020-01-01 04:51:00.000000  40.77898  -73.96925                  0.0   \n",
      "\n",
      "   snowfall  wind_speed_daily  \n",
      "0       0.0               0.0  \n",
      "1       0.0               0.0  \n",
      "2       0.0               0.0  \n",
      "3       0.0               0.0  \n",
      "4       0.0               0.0  \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define database URL and schema file\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "\n",
    "# Combine Yellow Taxi Parquet files\n",
    "yellow_taxi_files = glob.glob(\"yellow_taxi_cleaned_data/*.parquet\")\n",
    "taxi_data = pd.concat([pd.read_parquet(file) for file in yellow_taxi_files], ignore_index=True)\n",
    "print(f\"Loaded {len(taxi_data)} Yellow Taxi records.\")\n",
    "\n",
    "# Combine Uber Parquet files\n",
    "uber_files = glob.glob(\"fhvhv_cleaned_data/*.parquet\")\n",
    "uber_data = pd.concat([pd.read_parquet(file) for file in uber_files], ignore_index=True)\n",
    "print(f\"Loaded {len(uber_data)} Uber records.\")\n",
    "\n",
    "# Load Weather Data\n",
    "weather_data = pd.read_parquet(\"cleaned_weather_data.parquet\")\n",
    "\n",
    "# Split Weather Data\n",
    "hourly_columns = [\"date\", \"latitude\", \"longitude\", \"precipitation_hourly\", \"wind_speed_hourly\"]\n",
    "hourly_data = weather_data[hourly_columns].dropna(subset=[\"precipitation_hourly\", \"wind_speed_hourly\"])\n",
    "\n",
    "daily_columns = [\"date\", \"latitude\", \"longitude\", \"precipitation_daily\", \"snowfall\", \"wind_speed_daily\"]\n",
    "daily_data = weather_data[daily_columns].dropna(subset=[\"precipitation_daily\", \"snowfall\", \"wind_speed_daily\"])\n",
    "\n",
    "# Map table names to DataFrames\n",
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}\n",
    "\n",
    "# Create SQLite Engine\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Function to write DataFrames to tables\n",
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    with engine.connect() as connection:\n",
    "        for table_name, df in table_to_df_dict.items():\n",
    "            print(f\"Writing data to table: {table_name}\")\n",
    "            df.to_sql(table_name, connection, if_exists=\"replace\", index=False)\n",
    "            print(f\"Data written to table: {table_name}\")\n",
    "\n",
    "# Write DataFrames to SQLite database\n",
    "write_dataframes_to_table(map_table_name_to_dataframe)\n",
    "\"\"\"\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "# Verify data in the database\n",
    "with engine.connect() as connection:\n",
    "    for table in map_table_name_to_dataframe.keys():\n",
    "        # Wrap the query in text() to make it executable\n",
    "        query = text(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        result = connection.execute(query).fetchone()\n",
    "        print(f\"Table {table} contains {result[0]} records.\")\n",
    "\n",
    "    # Optionally, preview a sample of each table\n",
    "    for table in map_table_name_to_dataframe.keys():\n",
    "        query = text(f\"SELECT * FROM {table} LIMIT 5\")\n",
    "        sample = pd.read_sql(query, connection)\n",
    "        print(f\"Sample data from {table}:\")\n",
    "        print(sample)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23adde",
   "metadata": {},
   "source": [
    "### 3. Cleaning & Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb4265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "def clean_and_filter_data(df, zones_shapefile, bounding_box):\n",
    "    zones = gpd.read_file(zones_shapefile)\n",
    "    zones['centroid'] = zones['geometry'].centroid\n",
    "    zone_centroids = zones.set_index('LocationID')['centroid']\n",
    "    df['pickup_location'] = df['PULocationID'].map(zone_centroids)\n",
    "    df['dropoff_location'] = df['DOLocationID'].map(zone_centroids)\n",
    "    df = df.dropna(subset=['pickup_location', 'dropoff_location'])\n",
    "    min_lat, min_lon, max_lat, max_lon = bounding_box\n",
    "    df = df[\n",
    "        (df['pickup_location'].y >= min_lat) &\n",
    "        (df['pickup_location'].y <= max_lat) &\n",
    "        (df['pickup_location'].x >= min_lon) &\n",
    "        (df['pickup_location'].x <= max_lon)\n",
    "    ]\n",
    "    df.columns = df.columns.str.lower()\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "bounding_box = (40.560445, -74.242330, 40.908524, -73.717047)\n",
    "zones_shapefile = \"taxi_zones/taxi_zones.shp\"\n",
    "cleaned_data = clean_and_filter_data(sampled_data, zones_shapefile, bounding_box)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
