{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\"\"\"\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\"\"\"\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a8be8-6430-4ae5-ac26-f0534f906d4a",
   "metadata": {},
   "source": [
    "### 1. Downloading Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "391fde12-5f5e-47fe-ae34-0a9e3dc6b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Yellow Taxi files...\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet. Skipping.\n",
      "Downloading FHVHV files...\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet. Skipping.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet  after 3 attempts.\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet : HTTP 403\n",
      "Failed to download https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet  after 3 attempts.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet. Skipping.\n",
      "Invalid content type for https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet. Skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_parquet_links(url, regex_pattern):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "    return [link for link in links if re.search(regex_pattern, link)]\n",
    "\n",
    "def download_parquet_files(links, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for link in links:\n",
    "        file_name = os.path.join(save_dir, os.path.basename(link))\n",
    "        response = requests.get(link, stream=True)\n",
    "        with open(file_name, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "# Filter links in date range\n",
    "def filter_links_by_date(links, start_date, end_date):\n",
    "\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        match = re.search(r'(\\d{4})-(\\d{2})', link)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            date = datetime(year, month, 1)\n",
    "            if start_date <= date <= end_date:\n",
    "                filtered_links.append(link)\n",
    "    return filtered_links\n",
    "\n",
    "# Define date range\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2024, 8, 1)\n",
    "\n",
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "yellow_regex = r'yellow_tripdata_\\d{4}-\\d{2}\\.parquet'\n",
    "fhvhv_regex = r'fhvhv_tripdata_\\d{4}-\\d{2}\\.parquet'\n",
    "\n",
    "# Fetch links\n",
    "yellow_links = get_parquet_links(url, yellow_regex)\n",
    "fhvhv_links = get_parquet_links(url, fhvhv_regex)\n",
    "\n",
    "# Filter links by date\n",
    "yellow_links_filtered = filter_links_by_date(yellow_links, start_date, end_date)\n",
    "fhvhv_links_filtered = filter_links_by_date(fhvhv_links, start_date, end_date)\n",
    "\n",
    "# Download filtered files\n",
    "download_parquet_files(yellow_links_filtered, \"yellow_taxi_data\")\n",
    "download_parquet_files(fhvhv_links_filtered, \"fhvhv_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7914d1-4540-4db7-9692-94107d86a256",
   "metadata": {},
   "source": [
    "### 2. Sampling with Cochran's Formula\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d984bbc2-7713-4e3c-928e-d7c4da66506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a52e6-4a02-49e4-b17c-0c5ea71af16b",
   "metadata": {},
   "source": [
    "#### 2.1 Define the Sampling Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dfb0fb3-a5df-4f4c-a6ad-625692adde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cochran_sample_size(population_size, confidence_level=0.95, p=0.5, margin_of_error=0.05):\n",
    "    # Z-scores for common confidence levels\n",
    "    z_scores = {0.9: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    z = z_scores[confidence_level]\n",
    "    \n",
    "    # Cochran's initial sample size\n",
    "    n_0 = (z**2 * p * (1 - p)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust sample size for finite population\n",
    "    if population_size > 0:\n",
    "        n = n_0 / (1 + (n_0 - 1) / population_size)\n",
    "    else:\n",
    "        n = n_0  # Default to initial sample size if population size is unknown\n",
    "    \n",
    "    return math.ceil(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858550f-25c0-41e1-acb3-c38ab23dc2e4",
   "metadata": {},
   "source": [
    "#### 2.2 Sampling for Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b37e8db1-3649-4746-8050-715554bd1ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow Taxi - yellow_tripdata_2020-01.parquet: Population size = 6405008, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-02.parquet: Population size = 6299367, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-03.parquet: Population size = 3007687, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-04.parquet: Population size = 238073, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-05.parquet: Population size = 348415, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-06.parquet: Population size = 549797, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-07.parquet: Population size = 800412, Sample size = 384\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-08.parquet: Population size = 1007286, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-09.parquet: Population size = 1341017, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-10.parquet: Population size = 1681132, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-11.parquet: Population size = 1509000, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2020-12.parquet: Population size = 1461898, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2020-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-01.parquet: Population size = 1369769, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-02.parquet: Population size = 1371709, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-03.parquet: Population size = 1925152, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-04.parquet: Population size = 2171187, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-05.parquet: Population size = 2507109, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-06.parquet: Population size = 2834264, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-07.parquet: Population size = 2821746, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-08.parquet: Population size = 2788757, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-09.parquet: Population size = 2963793, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-10.parquet: Population size = 3463504, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-11.parquet: Population size = 3472949, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2021-12.parquet: Population size = 3214369, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2021-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-01.parquet: Population size = 2463931, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-02.parquet: Population size = 2979431, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-03.parquet: Population size = 3627882, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-04.parquet: Population size = 3599920, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-05.parquet: Population size = 3588295, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-06.parquet: Population size = 3558124, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-07.parquet: Population size = 3174394, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-08.parquet: Population size = 3152677, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-09.parquet: Population size = 3183767, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-10.parquet: Population size = 3675411, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-11.parquet: Population size = 3252717, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2022-12.parquet: Population size = 3399549, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2022-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-01.parquet: Population size = 3066766, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-02.parquet: Population size = 2913955, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-03.parquet: Population size = 3403766, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-04.parquet: Population size = 3288250, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-05.parquet: Population size = 3513649, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-06.parquet: Population size = 3307234, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-07.parquet: Population size = 2907108, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-08.parquet: Population size = 2824209, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-08.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-09.parquet: Population size = 2846722, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-09.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-10.parquet: Population size = 3522285, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-10.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-11.parquet: Population size = 3339715, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-11.parquet\n",
      "Yellow Taxi - yellow_tripdata_2023-12.parquet: Population size = 3376567, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2023-12.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-01.parquet: Population size = 2964624, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-01.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-02.parquet: Population size = 3007526, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-02.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-03.parquet: Population size = 3582628, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-03.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-04.parquet: Population size = 3514289, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-04.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-05.parquet: Population size = 3723833, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-05.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-06.parquet: Population size = 3539193, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-06.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-07.parquet: Population size = 3076903, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-07.parquet\n",
      "Yellow Taxi - yellow_tripdata_2024-08.parquet: Population size = 2979183, Sample size = 385\n",
      "Sampled data saved to: yellow_taxi_sampled_data/yellow_tripdata_2024-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-01.parquet: Population size = 20569368, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-02.parquet: Population size = 21725100, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-03.parquet: Population size = 13392928, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-04.parquet: Population size = 4312909, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-05.parquet: Population size = 6089999, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-06.parquet: Population size = 7555193, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-07.parquet: Population size = 9958454, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-08.parquet: Population size = 11096852, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-09.parquet: Population size = 12106669, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-10.parquet: Population size = 13268411, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-11.parquet: Population size = 11596865, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2020-12.parquet: Population size = 11637123, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2020-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-01.parquet: Population size = 11908468, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-02.parquet: Population size = 11613942, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-03.parquet: Population size = 14227393, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-04.parquet: Population size = 14111371, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-05.parquet: Population size = 14719171, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-06.parquet: Population size = 14961892, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-07.parquet: Population size = 15027174, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-08.parquet: Population size = 14499696, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-09.parquet: Population size = 14886055, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-10.parquet: Population size = 16545356, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-11.parquet: Population size = 16041639, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2021-12.parquet: Population size = 16054495, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2021-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-01.parquet: Population size = 14751591, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-02.parquet: Population size = 16019283, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-03.parquet: Population size = 18453548, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-04.parquet: Population size = 17752561, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-05.parquet: Population size = 18157335, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-06.parquet: Population size = 17780075, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-07.parquet: Population size = 17464619, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-08.parquet: Population size = 17185687, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-09.parquet: Population size = 17793551, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-10.parquet: Population size = 19306090, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-11.parquet: Population size = 18085896, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2022-12.parquet: Population size = 19665847, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2022-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-01.parquet: Population size = 18479031, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-02.parquet: Population size = 17960971, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-04.parquet: Population size = 19144903, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-05.parquet: Population size = 19847676, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-06.parquet: Population size = 19366619, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-07.parquet: Population size = 19132131, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-08.parquet: Population size = 18322150, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-08.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-09.parquet: Population size = 19851123, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-09.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-10.parquet: Population size = 20186330, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-10.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-11.parquet: Population size = 19269250, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-11.parquet\n",
      "FHVHV - fhvhv_tripdata_2023-12.parquet: Population size = 20516297, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2023-12.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-01.parquet: Population size = 19663930, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-01.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-02.parquet: Population size = 19359148, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-02.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-03.parquet: Population size = 21280788, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-03.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-04.parquet: Population size = 19733038, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-04.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-05.parquet: Population size = 20704538, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-05.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-06.parquet: Population size = 20123226, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-06.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-07.parquet: Population size = 19182934, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-07.parquet\n",
      "FHVHV - fhvhv_tripdata_2024-08.parquet: Population size = 19128392, Sample size = 385\n",
      "Sampled data saved to: fhvhv_sampled_data/fhvhv_tripdata_2024-08.parquet\n"
     ]
    }
   ],
   "source": [
    "# Function to sample a monthly dataset\n",
    "def sample_monthly_data(file_path, sample_size):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    sampled_df = df.sample(n=sample_size, random_state=42) \n",
    "    return sampled_df\n",
    "\n",
    "# Sampling logic for Yellow Taxi and FHVHV datasets\n",
    "def process_data(data_path_pattern, output_dir, dataset_type):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "    file_paths = sorted(glob.glob(data_path_pattern))  # Get all matching files\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Load dataset to calculate population size\n",
    "        population_size = len(pd.read_parquet(file_path))\n",
    "        \n",
    "        # Calculate sample size\n",
    "        sample_size = cochran_sample_size(population_size, confidence_level=0.95, margin_of_error=0.05)\n",
    "        print(f\"{dataset_type} - {os.path.basename(file_path)}: Population size = {population_size}, Sample size = {sample_size}\")\n",
    "        \n",
    "        # Sample data\n",
    "        sampled_data = sample_monthly_data(file_path, sample_size)\n",
    "        \n",
    "        # Save sampled data\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "        sampled_data.to_parquet(output_file)\n",
    "        print(f\"Sampled data saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# Process both datasets\n",
    "process_data(\"yellow_taxi_data/yellow_tripdata_202*.parquet\", \"yellow_taxi_sampled_data\", \"Yellow Taxi\")\n",
    "process_data(\"fhvhv_data/fhvhv_tripdata_202*.parquet\", \"fhvhv_sampled_data\", \"FHVHV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcfcbb-680e-4f0d-8c56-d830b03823b8",
   "metadata": {},
   "source": [
    "#### 2.3 Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21f807c-b1ab-4594-8b3f-5f8c191b5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import math\n",
    "import requests\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Coordinate Reference System\n",
    "CRS = 4326\n",
    "\n",
    "# (lat, lon) bounding box\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "# Define Uber-specific license number and base numbers\n",
    "uber_license_num = \"HV0003\"\n",
    "uber_base_numbers = [\n",
    "    \"B02877\", \"B02866\", \"B02882\", \"B02869\", \"B02617\", \"B02876\", \"B02865\", \"B02512\", \n",
    "    \"B02888\", \"B02864\", \"B02883\", \"B02875\", \"B02682\", \"B02880\", \"B02870\", \"B02404\", \n",
    "    \"B02598\", \"B02765\", \"B02879\", \"B02867\", \"B02878\", \"B02887\", \"B02872\", \"B02836\", \n",
    "    \"B02884\", \"B02835\", \"B02764\", \"B02889\", \"B02871\", \"B02395\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69bb9298-1b0b-4ec7-93e5-a1accbe2d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned weather data saved to: cleaned_weather_data.parquet\n"
     ]
    }
   ],
   "source": [
    "#!pip install fastparquet\n",
    "#!pip install pyarrow\n",
    "\n",
    "# Load Taxi Zone Shapefile\n",
    "taxi_zones = gpd.read_file(\"taxi_zones.shp\")\n",
    "\n",
    "# Project to a planar CRS before calculating centroids\n",
    "projected_taxi_zones = taxi_zones.to_crs(epsg=3857)  # Web Mercator projection\n",
    "projected_taxi_zones[\"centroid\"] = projected_taxi_zones.geometry.centroid\n",
    "\n",
    "# Reproject centroids back to geographic CRS for latitude/longitude extraction\n",
    "centroids_geographic = projected_taxi_zones[\"centroid\"].to_crs(epsg=4326)\n",
    "taxi_zones[\"latitude\"] = centroids_geographic.y\n",
    "taxi_zones[\"longitude\"] = centroids_geographic.x\n",
    "\n",
    "# Create lookup table for LocationID to Latitude/Longitude\n",
    "location_lookup = taxi_zones[[\"LocationID\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "# Load Weather Data\n",
    "weather_files = [\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2020_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2021_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2022_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2023_weather.csv\",\n",
    "    \"/Users/zhulilan/Documents/GitHub/final_project01/weather_data/2024_weather.csv\"\n",
    "]\n",
    "\n",
    "# Define data type mapping for weather columns\n",
    "dtype_mapping = {\n",
    "    \"HourlyPrecipitation\": \"string\",\n",
    "    \"DailySnowfall\": \"string\",\n",
    "    \"DailyPrecipitation\": \"string\",\n",
    "    \"LATITUDE\": \"float\",\n",
    "    \"LONGITUDE\": \"float\",\n",
    "    # Add other columns as needed\n",
    "}\n",
    "\n",
    "# Load and concatenate weather data with specified data types\n",
    "weather_data = pd.concat(\n",
    "    [pd.read_csv(file, dtype=dtype_mapping, low_memory=False) for file in weather_files],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Ensure numeric columns are converted to float after handling mixed types\n",
    "numeric_columns = [\"HourlyPrecipitation\", \"DailySnowfall\", \"DailyPrecipitation\"]\n",
    "for col in numeric_columns:\n",
    "    weather_data[col] = pd.to_numeric(weather_data[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "\n",
    "# Keep only the relevant columns\n",
    "weather_columns_to_keep = [\"DATE\", \"LATITUDE\", \"LONGITUDE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\", \"DailySnowfall\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\"]\n",
    "weather_data = weather_data[weather_columns_to_keep]\n",
    "\n",
    "# Rename columns to maintain consistency\n",
    "weather_data.rename(columns={\n",
    "    \"DATE\": \"date\",\n",
    "    \"LATITUDE\": \"latitude\",\n",
    "    \"LONGITUDE\": \"longitude\",\n",
    "    \"HourlyPrecipitation\": \"precipitation_hourly\",\n",
    "    \"DailyPrecipitation\": \"precipitation_daily\",\n",
    "    \"HourlyWindSpeed\": \"wind_speed_hourly\",\n",
    "    \"DailyAverageWindSpeed\": \"wind_speed_daily\",\n",
    "    \"DailySnowfall\": \"snowfall\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert 'T' values to 0.005 in relevant columns\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']].replace('T', 0.005)\n",
    "\n",
    "# Remove any alphabetic characters from numeric columns\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'snowfall']].replace(r'[a-zA-Z]', '', regex=True)\n",
    "\n",
    "# Convert date column to datetime\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "# Fill any missing values with zeros (for precipitation, wind speed, snowfall)\n",
    "weather_data[['precipitation_hourly', 'precipitation_daily', 'wind_speed_hourly', 'wind_speed_daily', 'snowfall']] = weather_data[['precipitation_hourly', 'precipitation_daily', 'wind_speed_hourly', 'wind_speed_daily', 'snowfall']].fillna(0)\n",
    "\n",
    "# Ensure correct data types\n",
    "weather_data['precipitation_hourly'] = weather_data['precipitation_hourly'].astype(float)\n",
    "weather_data['precipitation_daily'] = weather_data['precipitation_daily'].astype(float)\n",
    "weather_data['wind_speed_hourly'] = weather_data['wind_speed_hourly'].astype(float)\n",
    "weather_data['wind_speed_daily'] = weather_data['wind_speed_daily'].astype(float)\n",
    "weather_data['snowfall'] = weather_data['snowfall'].astype(float)\n",
    "weather_data['latitude'] = weather_data['latitude'].astype(float)\n",
    "weather_data['longitude'] = weather_data['longitude'].astype(float)\n",
    "\n",
    "# Save cleaned weather data to a separate file\n",
    "weather_output_file = \"cleaned_weather_data.parquet\"\n",
    "weather_data.to_parquet(weather_output_file)\n",
    "print(f\"Cleaned weather data saved to: {weather_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39879bfb-c592-4341-8912-26aff6909717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2020-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2021-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2022-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-08.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-09.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-10.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-11.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2023-12.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-01.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-02.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-03.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-04.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-05.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-06.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-07.parquet\n",
      "Cleaned data saved to: yellow_taxi_cleaned_data/yellow_tripdata_2024-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2020-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2021-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2022-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-08.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-09.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-10.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-11.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2023-12.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-01.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-02.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-03.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-04.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-05.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-06.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-07.parquet\n",
      "Cleaned data saved to: fhvhv_cleaned_data/fhvhv_tripdata_2024-08.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_and_filter_data(df, location_lookup, dataset_type, weather_data):\n",
    "    \"\"\"\n",
    "    Cleans and filters the ride data, and merges with weather data.\n",
    "    \"\"\"\n",
    "\n",
    "     # Filter Uber rides for FHVHV dataset\n",
    "    if dataset_type == \"FHVHV\":\n",
    "        # Normalize column values by converting to lowercase and stripping whitespaces\n",
    "        df['hvfhs_license_num'] = df['hvfhs_license_num'].str.lower().str.strip()\n",
    "        df['dispatching_base_num'] = df['dispatching_base_num'].str.lower().str.strip()\n",
    "        \n",
    "        # Define Uber-specific license number and base numbers\n",
    "        uber_license_num = \"hv0003\"  # Make sure it's in lowercase\n",
    "        \"\"\"\n",
    "        uber_base_numbers = [\n",
    "            \"b02877\", \"b02866\", \"b02882\", \"b02869\", \"b02617\", \"b02876\", \"b02865\", \"b02512\", \n",
    "            \"b02888\", \"b02864\", \"b02883\", \"b02875\", \"b02682\", \"b02880\", \"b02870\", \"b02404\", \n",
    "            \"b02598\", \"b02765\", \"b02879\", \"b02867\", \"b02878\", \"b02887\", \"b02872\", \"b02836\", \n",
    "            \"b02884\", \"b02835\", \"b02764\", \"b02889\", \"b02871\", \"b02395\"\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # Filter for Uber rides only\n",
    "        df = df[\n",
    "            (df['hvfhs_license_num'] == uber_license_num)]\n",
    "        #&\n",
    "          #  (df['dispatching_base_num'].isin(uber_base_numbers))\n",
    "        \n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "        print(\"Cleaned DataFrame (after filtering uber rides):\")\n",
    "        print(df.head())\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge LocationID with Latitude/Longitude\n",
    "    df = df.merge(location_lookup, left_on=\"PULocationID\", right_on=\"LocationID\", how=\"left\")\n",
    "    df = df.merge(location_lookup, left_on=\"DOLocationID\", right_on=\"LocationID\", how=\"left\", suffixes=(\"_pickup\", \"_dropoff\"))\n",
    "\n",
    "    \n",
    "    # Drop rows with missing latitude/longitude\n",
    "    df = df.dropna(subset=[\"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\"])\n",
    "    \"\"\"\n",
    "    print(\"Cleaned DataFrame (before removing unnecessary):\")\n",
    "    print(df.head())\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove unnecessary columns based on project queries\n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        columns_to_keep = [\n",
    "            \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "            \"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\", \n",
    "            \"fare_amount\", \"trip_distance\", \"total_amount\", \"tip_amount\"\n",
    "        ]\n",
    "        df = df[columns_to_keep]\n",
    "\n",
    "        # Rename columns for Yellow Taxi\n",
    "        df.rename(columns={\n",
    "            \"latitude_pickup\": \"latitude_pickup1\",\n",
    "            \"longitude_pickup\": \"longitude_pickup1\",\n",
    "            \"latitude_dropoff\": \"latitude_dropoff1\",\n",
    "            \"longitude_dropoff\": \"longitude_dropoff1\"\n",
    "        }, inplace=True)\n",
    "\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        columns_to_keep = [\n",
    "            \"pickup_datetime\", \"dropoff_datetime\",\n",
    "            \"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\", \n",
    "            \"base_passenger_fare\", \"trip_miles\", \"tolls\", \"tips\"\n",
    "        ]\n",
    "        df = df[columns_to_keep]\n",
    "\n",
    "        # Rename columns for FHVHV\n",
    "        df.rename(columns={\n",
    "            \"latitude_pickup\": \"latitude_pickup2\",\n",
    "            \"longitude_pickup\": \"longitude_pickup2\",\n",
    "            \"latitude_dropoff\": \"latitude_dropoff2\",\n",
    "            \"longitude_dropoff\": \"longitude_dropoff2\"\n",
    "        }, inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Remove trips with zero distance\n",
    "    #df = df[(df[\"latitude_pickup\"] != df[\"latitude_dropoff\"]) & (df[\"longitude_pickup\"] != df[\"longitude_dropoff\"])]\n",
    "\n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        df = df[\n",
    "            (df[\"latitude_pickup1\"] != df[\"latitude_dropoff1\"]) &\n",
    "            (df[\"longitude_pickup1\"] != df[\"longitude_dropoff1\"])\n",
    "        ]\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        df = df[\n",
    "            (df[\"latitude_pickup2\"] != df[\"latitude_dropoff2\"]) &\n",
    "            (df[\"longitude_pickup2\"] != df[\"longitude_dropoff2\"])\n",
    "        ]\n",
    "    #print(f\"Columns available before filtering: {df.columns.tolist()}\")\n",
    "    \n",
    "\n",
    "    # Remove trips outside of the NYC bounding box\n",
    "    def is_within_bounding_box(lat, lon, bounding_box):\n",
    "        return (bounding_box[0][0] <= lat <= bounding_box[1][0]) and (bounding_box[0][1] <= lon <= bounding_box[1][1])\n",
    "    \n",
    "    if dataset_type == \"Yellow Taxi\":\n",
    "        df = df[df.apply(lambda row: is_within_bounding_box(row[\"latitude_pickup1\"], row[\"longitude_pickup1\"], NEW_YORK_BOX_COORDS) and\n",
    "                                    is_within_bounding_box(row[\"latitude_dropoff1\"], row[\"longitude_dropoff1\"], NEW_YORK_BOX_COORDS), axis=1)]\n",
    "    elif dataset_type == \"FHVHV\":\n",
    "        df = df[df.apply(lambda row: is_within_bounding_box(row[\"latitude_pickup2\"], row[\"longitude_pickup2\"], NEW_YORK_BOX_COORDS) and\n",
    "                                    is_within_bounding_box(row[\"latitude_dropoff2\"], row[\"longitude_dropoff2\"], NEW_YORK_BOX_COORDS), axis=1)]\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Paths for Yellow Taxi and FHVHV data\n",
    "yellow_taxi_path_pattern = \"yellow_taxi_sampled_data/yellow_tripdata_202*.parquet\"\n",
    "fhvhv_path_pattern = \"fhvhv_sampled_data/fhvhv_tripdata_202*.parquet\"\n",
    "\n",
    "# Output directories\n",
    "yellow_taxi_output_dir = \"yellow_taxi_cleaned_data\"\n",
    "fhvhv_output_dir = \"fhvhv_cleaned_data\"\n",
    "\n",
    "# Process both datasets\n",
    "def process_cleaning(data_path_pattern, output_dir, dataset_type, weather_data):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "    file_paths = sorted(glob.glob(data_path_pattern))  # Get all matching files\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        cleaned_data = clean_and_filter_data(df, location_lookup, dataset_type, weather_data)\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "        cleaned_data.to_parquet(output_file)\n",
    "        print(f\"Cleaned data saved to: {output_file}\")\n",
    "\n",
    "# Run the cleaning process\n",
    "process_cleaning(yellow_taxi_path_pattern, yellow_taxi_output_dir, \"Yellow Taxi\", weather_data)\n",
    "process_cleaning(fhvhv_path_pattern, fhvhv_output_dir, \"FHVHV\", weather_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(taxi_page):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(all_urls):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(url):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(url):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls):\n",
    "    all_uber_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_uber_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    uber_data = pd.contact(all_uber_dataframes)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    all_urls = get_all_urls_from_tlc_page(TLC_URL)\n",
    "    all_parquet_urls = find_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23adde",
   "metadata": {},
   "source": [
    "### 3. Cleaning & Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb4265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "def clean_and_filter_data(df, zones_shapefile, bounding_box):\n",
    "    zones = gpd.read_file(zones_shapefile)\n",
    "    zones['centroid'] = zones['geometry'].centroid\n",
    "    zone_centroids = zones.set_index('LocationID')['centroid']\n",
    "    df['pickup_location'] = df['PULocationID'].map(zone_centroids)\n",
    "    df['dropoff_location'] = df['DOLocationID'].map(zone_centroids)\n",
    "    df = df.dropna(subset=['pickup_location', 'dropoff_location'])\n",
    "    min_lat, min_lon, max_lat, max_lon = bounding_box\n",
    "    df = df[\n",
    "        (df['pickup_location'].y >= min_lat) &\n",
    "        (df['pickup_location'].y <= max_lat) &\n",
    "        (df['pickup_location'].x >= min_lon) &\n",
    "        (df['pickup_location'].x <= max_lon)\n",
    "    ]\n",
    "    df.columns = df.columns.str.lower()\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "bounding_box = (40.560445, -74.242330, 40.908524, -73.717047)\n",
    "zones_shapefile = \"taxi_zones/taxi_zones.shp\"\n",
    "cleaned_data = clean_and_filter_data(sampled_data, zones_shapefile, bounding_box)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
